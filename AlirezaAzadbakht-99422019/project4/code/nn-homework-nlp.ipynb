{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-07T07:31:09.867594Z","iopub.execute_input":"2021-07-07T07:31:09.86822Z","iopub.status.idle":"2021-07-07T07:31:09.880769Z","shell.execute_reply.started":"2021-07-07T07:31:09.86811Z","shell.execute_reply":"2021-07-07T07:31:09.879976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport string\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import wordnet\nimport itertools\nimport re\n\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import BertModel, BertTokenizer\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\ndef make_report(y_pred , y_true):\n    print (\"\")\n    print (\"Classification Report: \")\n    print (classification_report(y_true, y_pred))\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plot_confusion_matrix(conf_mat=cm)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:21:18.306153Z","iopub.execute_input":"2021-07-07T09:21:18.306574Z","iopub.status.idle":"2021-07-07T09:21:18.316362Z","shell.execute_reply.started":"2021-07-07T09:21:18.30654Z","shell.execute_reply":"2021-07-07T09:21:18.31532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv', index_col='id')\nvalidation = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv', index_col='id')\nprint(data.shape)\nprint(validation.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:13.339447Z","iopub.execute_input":"2021-07-07T07:31:13.340254Z","iopub.status.idle":"2021-07-07T07:31:13.444398Z","shell.execute_reply.started":"2021-07-07T07:31:13.340197Z","shell.execute_reply":"2021-07-07T07:31:13.443605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"data.target.hist()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:13.445888Z","iopub.execute_input":"2021-07-07T07:31:13.446408Z","iopub.status.idle":"2021-07-07T07:31:13.651706Z","shell.execute_reply.started":"2021-07-07T07:31:13.446371Z","shell.execute_reply":"2021-07-07T07:31:13.650876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = data.shape[0]\nprint('location ratio',data[~data.location.isnull()].shape[0]/n)\nprint('keyword ratio',data[~data.keyword.isnull()].shape[0]/n)\nprint('keyword and location ratio',data[(~data.keyword.isnull()) & (~data.location.isnull())].shape[0]/n)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:13.653061Z","iopub.execute_input":"2021-07-07T07:31:13.653591Z","iopub.status.idle":"2021-07-07T07:31:13.671235Z","shell.execute_reply.started":"2021-07-07T07:31:13.653556Z","shell.execute_reply":"2021-07-07T07:31:13.669953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[~data.location.isnull()].target.hist()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:13.673001Z","iopub.execute_input":"2021-07-07T07:31:13.673433Z","iopub.status.idle":"2021-07-07T07:31:13.840694Z","shell.execute_reply.started":"2021-07-07T07:31:13.673391Z","shell.execute_reply":"2021-07-07T07:31:13.839891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[~data.keyword.isnull()].target.hist()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:13.842762Z","iopub.execute_input":"2021-07-07T07:31:13.843239Z","iopub.status.idle":"2021-07-07T07:31:14.000396Z","shell.execute_reply.started":"2021-07-07T07:31:13.843184Z","shell.execute_reply":"2021-07-07T07:31:13.999256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data.keyword.unique())","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:14.002472Z","iopub.execute_input":"2021-07-07T07:31:14.002786Z","iopub.status.idle":"2021-07-07T07:31:14.010092Z","shell.execute_reply.started":"2021-07-07T07:31:14.002756Z","shell.execute_reply":"2021-07-07T07:31:14.009048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.keyword.hist()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:14.011617Z","iopub.execute_input":"2021-07-07T07:31:14.011996Z","iopub.status.idle":"2021-07-07T07:31:17.005275Z","shell.execute_reply.started":"2021-07-07T07:31:14.01196Z","shell.execute_reply":"2021-07-07T07:31:17.004182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data.location.unique())","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:17.006811Z","iopub.execute_input":"2021-07-07T07:31:17.007174Z","iopub.status.idle":"2021-07-07T07:31:17.015621Z","shell.execute_reply.started":"2021-07-07T07:31:17.007138Z","shell.execute_reply":"2021-07-07T07:31:17.014538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{}},{"cell_type":"code","source":"def clean_abbreviation(text):\n    text = re.sub(r\"he's\", \"he is\",  text)\n    text = re.sub(r\"there's\", \"there is\",  text)\n    text = re.sub(r\"We're\", \"We are\",  text)\n    text = re.sub(r\"That's\", \"That is\",  text)\n    text = re.sub(r\"won't\", \"will not\",  text)\n    text = re.sub(r\"they're\", \"they are\",  text)\n    text = re.sub(r\"Can't\", \"Cannot\",  text)\n    text = re.sub(r\"wasn't\", \"was not\",  text)\n    text = re.sub(r\"aren't\", \"are not\",  text)\n    text = re.sub(r\"isn't\", \"is not\",  text)\n    text = re.sub(r\"What's\", \"What is\",  text)\n    text = re.sub(r\"i'd\", \"I would\",  text)\n    text = re.sub(r\"should've\", \"should have\",  text)\n    text = re.sub(r\"where's\", \"where is\",  text)\n    text = re.sub(r\"we'd\", \"we would\",  text)\n    text = re.sub(r\"i'll\", \"I will\",  text)\n    text = re.sub(r\"weren't\", \"were not\",  text)\n    text = re.sub(r\"They're\", \"They are\",  text)\n    text = re.sub(r\"let's\", \"let us\",  text)\n    text = re.sub(r\"it's\", \"it is\",  text)\n    text = re.sub(r\"can't\", \"cannot\",  text)\n    text = re.sub(r\"don't\", \"do not\",  text)\n    text = re.sub(r\"you're\", \"you are\",  text)\n    text = re.sub(r\"i've\", \"I have\",  text)\n    text = re.sub(r\"that's\", \"that is\",  text)\n    text = re.sub(r\"i'll\", \"I will\",  text)\n    text = re.sub(r\"doesn't\", \"does not\",  text)\n    text = re.sub(r\"i'd\", \"I would\",  text)\n    text = re.sub(r\"didn't\", \"did not\",  text)\n    text = re.sub(r\"ain't\", \"am not\",  text)\n    text = re.sub(r\"you'll\", \"you will\",  text)\n    text = re.sub(r\"I've\", \"I have\",  text)\n    text = re.sub(r\"Don't\", \"do not\",  text)\n    text = re.sub(r\"I'll\", \"I will\",  text)\n    text = re.sub(r\"I'd\", \"I would\",  text)\n    text = re.sub(r\"Let's\", \"Let us\",  text)\n    text = re.sub(r\"you'd\", \"You would\",  text)\n    text = re.sub(r\"It's\", \"It is\",  text)\n    text = re.sub(r\"Ain't\", \"am not\",  text)\n    text = re.sub(r\"Haven't\", \"Have not\",  text)\n    text = re.sub(r\"Could've\", \"Could have\",  text)\n    text = re.sub(r\"youve\", \"you have\",  text)\n    text = re.sub(r\"haven't\", \"have not\",  text)\n    text = re.sub(r\"hasn't\", \"has not\",  text)\n    text = re.sub(r\"There's\", \"There is\",  text)\n    text = re.sub(r\"He's\", \"He is\",  text)\n    text = re.sub(r\"It's\", \"It is\",  text)\n    text = re.sub(r\"You're\", \"You are\",  text)\n    text = re.sub(r\"I'M\", \"I am\",  text)\n    text = re.sub(r\"shouldn't\", \"should not\",  text)\n    text = re.sub(r\"wouldn't\", \"would not\",  text)\n    text = re.sub(r\"i'm\", \"I am\",  text)\n    text = re.sub(r\"I'm\", \"I am\",  text)\n    text = re.sub(r\"Isn't\", \"is not\",  text)\n    text = re.sub(r\"Here's\", \"Here is\",  text)\n    text = re.sub(r\"you've\", \"you have\",  text)\n    text = re.sub(r\"we're\", \"we are\",  text)\n    text = re.sub(r\"what's\", \"what is\",  text)\n    text = re.sub(r\"couldn't\", \"could not\",  text)\n    text = re.sub(r\"we've\", \"we have\",  text)\n    text = re.sub(r\"who's\", \"who is\",  text)\n    text = re.sub(r\"y'all\", \"you all\",  text)\n    text = re.sub(r\"would've\", \"would have\",  text)\n    text = re.sub(r\"it'll\", \"it will\",  text)\n    text = re.sub(r\"we'll\", \"we will\",  text)\n    text = re.sub(r\"We've\", \"We have\",  text)\n    text = re.sub(r\"he'll\", \"he will\",  text)\n    text = re.sub(r\"Y'all\", \"You all\",  text)\n    text = re.sub(r\"Weren't\", \"Were not\",  text)\n    text = re.sub(r\"Didn't\", \"Did not\",  text)\n    text = re.sub(r\"they'll\", \"they will\",  text)\n    text = re.sub(r\"they'd\", \"they would\",  text)\n    text = re.sub(r\"DON'T\", \"DO NOT\",  text)\n    text = re.sub(r\"they've\", \"they have\",  text)\n\n    text = re.sub(r\"tnwx\", \"Tennessee Weather\",  text)\n    text = re.sub(r\"azwx\", \"Arizona Weather\",  text)  \n    text = re.sub(r\"alwx\", \"Alabama Weather\",  text)\n    text = re.sub(r\"wordpressdotcom\", \"wordpress\",  text)      \n    text = re.sub(r\"gawx\", \"Georgia Weather\",  text)  \n    text = re.sub(r\"scwx\", \"South Carolina Weather\",  text)  \n    text = re.sub(r\"cawx\", \"California Weather\",  text)\n    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\",  text) \n    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\",  text)\n    text = re.sub(r\"okwx\", \"Oklahoma City Weather\",  text)\n    text = re.sub(r\"arwx\", \"Arkansas Weather\",  text)  \n    text = re.sub(r\"lmao\", \"laughing my ass off\",  text)  \n    text = re.sub(r\"amirite\", \"am I right\",  text)\n\n    text = re.sub(r\"w/e\", \"whatever\",  text)\n    text = re.sub(r\"w/\", \"with\",  text)\n    text = re.sub(r\"USAgov\", \"USA government\",  text)\n    text = re.sub(r\"recentlu\", \"recently\",  text)\n    text = re.sub(r\"Ph0tos\", \"Photos\",  text)\n    text = re.sub(r\"exp0sed\", \"exposed\",  text)\n    text = re.sub(r\"<3\", \"love\",  text)\n    text = re.sub(r\"amageddon\", \"armageddon\",  text)\n    text = re.sub(r\"Trfc\", \"Traffic\",  text)\n    text = re.sub(r\"WindStorm\", \"Wind Storm\",  text)\n    text = re.sub(r\"16yr\", \"16 year\",  text)\n    text = re.sub(r\"TRAUMATISED\", \"traumatized\",  text)\n\n    text = re.sub(r\"IranDeal\", \"Iran Deal\",  text)\n    text = re.sub(r\"ArianaGrande\", \"Ariana Grande\",  text)\n    text = re.sub(r\"camilacabello97\", \"camila cabello\",  text) \n    text = re.sub(r\"RondaRousey\", \"Ronda Rousey\",  text)     \n    text = re.sub(r\"MTVHottest\", \"MTV Hottest\",  text)\n    text = re.sub(r\"TrapMusic\", \"Trap Music\",  text)\n    text = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\",  text)\n    text = re.sub(r\"PantherAttack\", \"Panther Attack\",  text)\n    text = re.sub(r\"StrategicPatience\", \"Strategic Patience\",  text)\n    text = re.sub(r\"socialnews\", \"social news\",  text)\n    text = re.sub(r\"IDPs:\", \"Internally Displaced People :\",  text)\n    text = re.sub(r\"ArtistsUnited\", \"Artists United\",  text)\n    text = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\",  text)\n    text = re.sub(r\"jimmyfallon\", \"jimmy fallon\",  text)\n    text = re.sub(r\"justinbieber\", \"justin bieber\",  text)  \n    text = re.sub(r\"Time2015\", \"Time 2015\",  text)\n    text = re.sub(r\"djicemoon\", \"dj icemoon\",  text)\n    text = re.sub(r\"LivingSafely\", \"Living Safely\",  text)\n    text = re.sub(r\"FIFA16\", \"Fifa 2016\",  text)\n    text = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\",  text)\n    text = re.sub(r\"bbcnews\", \"bbc news\",  text)\n    text = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\",  text)\n    text = re.sub(r\"c4news\", \"c4 news\",  text)\n    text = re.sub(r\"MUDSLIDE\", \"mudslide\",  text)\n    text = re.sub(r\"NoSurrender\", \"No Surrender\",  text)\n    text = re.sub(r\"NotExplained\", \"Not Explained\",  text)\n    text = re.sub(r\"greatbritishbakeoff\", \"great british bake off\",  text)\n    text = re.sub(r\"LondonFire\", \"London Fire\",  text)\n    text = re.sub(r\"KOTAWeather\", \"KOTA Weather\",  text)\n    text = re.sub(r\"LuchaUnderground\", \"Lucha Underground\",  text)\n    text = re.sub(r\"KOIN6News\", \"KOIN 6 News\",  text)\n    text = re.sub(r\"LiveOnK2\", \"Live On K2\",  text)\n    text = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\",  text)\n    text = re.sub(r\"nikeplus\", \"nike plus\",  text)\n    text = re.sub(r\"david_cameron\", \"David Cameron\",  text)\n    text = re.sub(r\"peterjukes\", \"Peter Jukes\",  text)\n    text = re.sub(r\"MikeParrActor\", \"Michael Parr\",  text)\n    text = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\",  text)\n    text = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\",  text)\n    text = re.sub(r\"realmandyrain\", \"Mandy Rain\",  text)\n    text = re.sub(r\"GraysonDolan\", \"Grayson Dolan\",  text)\n    text = re.sub(r\"ApolloBrown\", \"Apollo Brown\",  text)\n    text = re.sub(r\"saddlebrooke\", \"Saddlebrooke\",  text)\n    text = re.sub(r\"TontitownGrape\", \"Tontitown Grape\",  text)\n    text = re.sub(r\"AbbsWinston\", \"Abbs Winston\",  text)\n    text = re.sub(r\"ShaunKing\", \"Shaun King\",  text)\n    text = re.sub(r\"MeekMill\", \"Meek Mill\",  text)\n    text = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\",  text)\n    text = re.sub(r\"GRupdates\", \"GR updates\",  text)\n    text = re.sub(r\"SouthDowns\", \"South Downs\",  text)\n    text = re.sub(r\"braininjury\", \"brain injury\",  text)\n    text = re.sub(r\"auspol\", \"Australian politics\",  text)\n    text = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\",  text)\n    text = re.sub(r\"calgaryweather\", \"Calgary Weather\",  text)\n    text = re.sub(r\"weallheartonedirection\", \"we all heart one direction\",  text)\n    text = re.sub(r\"edsheeran\", \"Ed Sheeran\",  text)\n    text = re.sub(r\"TrueHeroes\", \"True Heroes\",  text)\n    text = re.sub(r\"ComplexMag\", \"Complex Magazine\",  text)\n    text = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\",  text)\n    text = re.sub(r\"CityofCalgary\", \"City of Calgary\",  text)\n    text = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\",  text)\n    text = re.sub(r\"SummerFate\", \"Summer Fate\",  text)\n    text = re.sub(r\"RAmag\", \"Royal Academy Magazine\",  text)\n    text = re.sub(r\"offers2go\", \"offers to go\",  text)\n    text = re.sub(r\"ModiMinistry\", \"Modi Ministry\",  text)\n    text = re.sub(r\"TAXIWAYS\", \"taxi ways\",  text)\n    text = re.sub(r\"Calum5SOS\", \"Calum Hood\",  text)\n    text = re.sub(r\"JamesMelville\", \"James Melville\",  text)\n    text = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\",  text)\n    text = re.sub(r\" textLikeItsSeptember11th2001\", \" text like it is september 11th 2001\",  text)\n    text = re.sub(r\"cbplawyers\", \"cbp lawyers\",  text)\n    text = re.sub(r\"fewmore texts\", \"few more  texts\",  text)\n    text = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\",  text)\n    text = re.sub(r\"NASAHurricane\", \"NASA Hurricane\",  text)\n    text = re.sub(r\"onlinecommunities\", \"online communities\",  text)\n    text = re.sub(r\"humanconsumption\", \"human consumption\",  text)\n    text = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\",  text)\n    text = re.sub(r\"Meat-Loving\", \"Meat Loving\",  text)\n    text = re.sub(r\"facialabuse\", \"facial abuse\",  text)\n    text = re.sub(r\"LakeCounty\", \"Lake County\",  text)\n    text = re.sub(r\"BeingAuthor\", \"Being Author\",  text)\n    text = re.sub(r\"withheavenly\", \"with heavenly\",  text)\n    text = re.sub(r\"thankU\", \"thank you\",  text)\n    text = re.sub(r\"iTunesMusic\", \"iTunes Music\",  text)\n    text = re.sub(r\"OffensiveContent\", \"Offensive Content\",  text)\n    text = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\",  text)\n    text = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\",  text)\n    text = re.sub(r\"NASASolarSystem\", \"NASA Solar System\",  text)\n    text = re.sub(r\"animalrescue\", \"animal rescue\",  text)\n    text = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\",  text)\n    text = re.sub(r\"Throwingknifes\", \"Throwing knives\",  text)\n    text = re.sub(r\"GodsLove\", \"God's Love\",  text)\n    text = re.sub(r\"bookboost\", \"book boost\",  text)\n    text = re.sub(r\"ibooklove\", \"I book love\",  text)\n    text = re.sub(r\"NestleIndia\", \"Nestle India\",  text)\n    text = re.sub(r\"realDonaldTrump\", \"Donald Trump\",  text)\n    text = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\",  text)\n    text = re.sub(r\"CecilTheLion\", \"Cecil The Lion\",  text)\n    text = re.sub(r\"weathernetwork\", \"weather network\",  text)\n    text = re.sub(r\"GOPDebate\", \"GOP Debate\",  text)\n    text = re.sub(r\"RickPerry\", \"Rick Perry\",  text)\n    text = re.sub(r\"frontpage\", \"front page\",  text)\n    text = re.sub(r\"NewsIn texts\", \"News In  texts\",  text)\n    text = re.sub(r\"ViralSpell\", \"Viral Spell\",  text)\n    text = re.sub(r\"til_now\", \"until now\",  text)\n    text = re.sub(r\"volcanoinRussia\", \"volcano in Russia\",  text)\n    text = re.sub(r\"ZippedNews\", \"Zipped News\",  text)\n    text = re.sub(r\"MicheleBachman\", \"Michele Bachman\",  text)\n    text = re.sub(r\"53inch\", \"53 inch\",  text)\n    text = re.sub(r\"KerrickTrial\", \"Kerrick Trial\",  text)\n    text = re.sub(r\"abstorm\", \"Alberta Storm\",  text)\n    text = re.sub(r\"Beyhive\", \"Beyonce hive\",  text)\n    text = re.sub(r\"RockyFire\", \"Rocky Fire\",  text)\n    text = re.sub(r\"Listen/Buy\", \"Listen / Buy\",  text)\n    text = re.sub(r\"ArtistsUnited\", \"Artists United\",  text)\n    text = re.sub(r\"ENGvAUS\", \"England vs Australia\",  text)\n    text = re.sub(r\"ScottWalker\", \"Scott Walker\",  text)\n    return text\n    \ndef lower_stopwords_punctuation(text):\n    text = text.lower()\n    text = ' '.join([w for w in word_tokenize(text) if not w in set(stopwords.words('english'))])\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = ' '.join(text.split())\n    return text\n\ndef simple_cleaning(text):\n    text=''.join(i for i, _ in itertools.groupby(text))    \n    pattern = '[0-9]+'\n    text=re.sub(pattern, ' isnumber ', text) \n    text = re.sub('[^a-zA-Z]', ' ',  text)\n    return text\n\ndef get_wordnet_pos(treebank_tag):\n    lemmatizer = WordNetLemmatizer()\n    name = treebank_tag[0]\n    treebank_tag = treebank_tag[1]\n    if treebank_tag.startswith('J'):\n        return lemmatizer.lemmatize(name, wordnet.ADJ)\n    elif treebank_tag.startswith('V'):\n        return lemmatizer.lemmatize(name, wordnet.VERB)\n    elif treebank_tag.startswith('N'):\n        return lemmatizer.lemmatize(name, wordnet.NOUN)\n    elif treebank_tag.startswith('R'):\n        return lemmatizer.lemmatize(name, wordnet.ADV)\n    else:\n        return lemmatizer.lemmatize(name, wordnet.NOUN)\n\ndef lemma(text):\n    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n    pos = nltk.pos_tag(tokenizer.tokenize(text))\n    return ' '.join(map(get_wordnet_pos, pos))\n\ndef cleaning_pipeline(text):\n    text = clean_abbreviation(text)\n    text = lower_stopwords_punctuation(text)\n    text = simple_cleaning(text)\n    text = lemma(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:17.018115Z","iopub.execute_input":"2021-07-07T07:31:17.018599Z","iopub.status.idle":"2021-07-07T07:31:17.095996Z","shell.execute_reply.started":"2021-07-07T07:31:17.018552Z","shell.execute_reply":"2021-07-07T07:31:17.09517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata['text_clean'] = data.text.apply(lambda x: cleaning_pipeline(x))\nvalidation['text_clean'] = validation.text.apply(lambda x: cleaning_pipeline(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:31:17.097173Z","iopub.execute_input":"2021-07-07T07:31:17.097601Z","iopub.status.idle":"2021-07-07T07:32:14.979844Z","shell.execute_reply.started":"2021-07-07T07:31:17.097558Z","shell.execute_reply":"2021-07-07T07:32:14.978834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:32:14.981238Z","iopub.execute_input":"2021-07-07T07:32:14.981554Z","iopub.status.idle":"2021-07-07T07:32:14.996233Z","shell.execute_reply.started":"2021-07-07T07:32:14.981521Z","shell.execute_reply":"2021-07-07T07:32:14.994929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data.drop('target', axis = 1), data.target, test_size=0.2, random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:32:14.997977Z","iopub.execute_input":"2021-07-07T07:32:14.998345Z","iopub.status.idle":"2021-07-07T07:32:15.017087Z","shell.execute_reply.started":"2021-07-07T07:32:14.998313Z","shell.execute_reply":"2021-07-07T07:32:15.015877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padding_zero(vec, max_length):\n    if len(vec) < max_length:\n        vec = np.pad(vec, ( (0, max_length-len(vec)), (0,0) ), 'constant')\n    elif len(vec) > max_length:\n        vec = vec[:max_length]\n    return vec\n\ndef embed_data(data):\n    model = BertModel.from_pretrained('bert-base-uncased')\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    text_data = list(data[\"text_clean\"].values)\n    train_lst = list()\n\n    for i in tqdm(range(0, len(text_data))):\n        input_ids = torch.tensor(tokenizer.encode(text_data[i], add_special_tokens=True, max_length=100)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0][0]\n        bert_arr = padding_zero(last_hidden_states.detach().numpy(), 64)\n        train_lst += [bert_arr]\n    train_arr = np.array(train_lst)\n    return train_arr\n\ntrain_arr = embed_data(x_train)\ntest_arr = embed_data(x_test)\nval_arr = embed_data(validation)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:32:15.018432Z","iopub.execute_input":"2021-07-07T07:32:15.018714Z","iopub.status.idle":"2021-07-07T07:49:00.046794Z","shell.execute_reply.started":"2021-07-07T07:32:15.018687Z","shell.execute_reply":"2021-07-07T07:49:00.0454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 128\n\nclass MyDataSet(torch.utils.data.Dataset):\n    def __init__(self, x, y):\n        self.x = torch.tensor(x)\n        self.y = torch.tensor(y).long()\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        return(self.x[idx], self.y[idx])\n    \ntrain_dataset = MyDataSet(train_arr, y_train.values)\ntest_dataset = MyDataSet(test_arr, y_test.values)\nvalidation_dataset = MyDataSet(val_arr, np.zeros(validation.shape[0]))\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False)\nvalidation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:49:00.049624Z","iopub.execute_input":"2021-07-07T07:49:00.050118Z","iopub.status.idle":"2021-07-07T07:49:02.033057Z","shell.execute_reply.started":"2021-07-07T07:49:00.050041Z","shell.execute_reply":"2021-07-07T07:49:02.031869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"class TransformerClassifier(torch.nn.Module):\n    def __init__(self):\n        super(TransformerClassifier, self).__init__()\n        encoder = nn.TransformerEncoderLayer(768, 32, dim_feedforward=256) \n        self.encoder = nn.TransformerEncoder(encoder, 4) \n        self.flatten = torch.nn.Sequential(nn.MaxPool2d((3, 3)), torch.nn.Flatten())\n        self.linear = nn.Linear(5376, 2)\n    \n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.flatten(x)\n        x = self.linear(x)\n        return x\nmodel = TransformerClassifier()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:09:24.703345Z","iopub.execute_input":"2021-07-07T10:09:24.703742Z","iopub.status.idle":"2021-07-07T10:09:24.751882Z","shell.execute_reply.started":"2021-07-07T10:09:24.703708Z","shell.execute_reply":"2021-07-07T10:09:24.751068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"%%time\nN_EPOCHS = 10\ndevice = 'cpu'\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\nprint('started!')\nfor epoch in range(N_EPOCHS):\n    #Train\n    train_batch_loss = 0\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        x = batch[0].to(device)\n        y = batch[1].to(device)\n        if step%10==1:\n            print(step)\n        optimizer.zero_grad()\n        outputs = model(x)    \n        loss = criterion(outputs, y)\n        train_batch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    #Validation\n    test_batch_loss = 0\n    model.eval()\n    with torch.no_grad():\n        for step, batch in enumerate(test_dataloader):\n            x = batch[0].to(device)\n            y = batch[1].to(device)\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            test_batch_loss += loss.item()\n\n    print('{}/{} train loss: {} test loss: {}'.format(epoch+1, N_EPOCHS,\n                                                            train_batch_loss / len(train_dataloader),\n                                                            test_batch_loss / len(test_dataloader)))\n\ntorch.save(model.state_dict(), './new.model')","metadata":{"execution":{"iopub.status.busy":"2021-07-07T07:49:02.090054Z","iopub.execute_input":"2021-07-07T07:49:02.090414Z","iopub.status.idle":"2021-07-07T08:45:27.448715Z","shell.execute_reply.started":"2021-07-07T07:49:02.09037Z","shell.execute_reply":"2021-07-07T08:45:27.447219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\ntrue = []\nwith torch.no_grad():\n    for step, batch in enumerate(test_dataloader):\n        x = batch[0].to(device)\n        y = batch[1].to(device)\n        outputs = model(x)\n        pred+=(torch.argmax(outputs, axis = 1).detach().numpy().tolist())\n        true+=(y.detach().numpy().tolist())","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:18:59.012532Z","iopub.execute_input":"2021-07-07T09:18:59.013028Z","iopub.status.idle":"2021-07-07T09:19:23.824595Z","shell.execute_reply.started":"2021-07-07T09:18:59.012998Z","shell.execute_reply":"2021-07-07T09:19:23.823698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_report(y_pred=pred, y_true=true)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:21:25.091165Z","iopub.execute_input":"2021-07-07T09:21:25.091747Z","iopub.status.idle":"2021-07-07T09:21:25.224277Z","shell.execute_reply.started":"2021-07-07T09:21:25.091705Z","shell.execute_reply":"2021-07-07T09:21:25.223113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = []\nwith torch.no_grad():\n    for step, batch in enumerate(validation_dataloader):\n        x = batch[0].to(device)\n        y = batch[1].to(device)\n        outputs = model(x)\n        pred+=(torch.argmax(outputs, axis = 1).detach().numpy().tolist())","metadata":{"execution":{"iopub.status.busy":"2021-07-07T09:42:00.229599Z","iopub.execute_input":"2021-07-07T09:42:00.230051Z","iopub.status.idle":"2021-07-07T09:42:53.672578Z","shell.execute_reply.started":"2021-07-07T09:42:00.230004Z","shell.execute_reply":"2021-07-07T09:42:53.671751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'id':validation.index, 'target':pred}).to_csv('./submition1.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T10:02:47.810313Z","iopub.execute_input":"2021-07-07T10:02:47.81069Z","iopub.status.idle":"2021-07-07T10:02:47.831708Z","shell.execute_reply.started":"2021-07-07T10:02:47.810658Z","shell.execute_reply":"2021-07-07T10:02:47.830554Z"},"trusted":true},"execution_count":null,"outputs":[]}]}