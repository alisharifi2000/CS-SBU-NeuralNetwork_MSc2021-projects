{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image-coloring.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraMousakhani/CS-SBU-NeuralNetwork_MSc2021-projects/blob/main/ZahraMousakhani-99422187/project3/image_coloring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWDiplzMLWoG"
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/drive/MyDrive/paintingzip.zip', 'r')\n",
        "zip_ref.extractall('paintingedit')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgH_kLbHZdVr",
        "outputId": "cead586d-79da-42a8-d0c1-887f1fe28d85"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD2JEPf6Lwh2"
      },
      "source": [
        "# Move data into training and validation directories\n",
        "import os\n",
        "os.makedirs('images/train/class/', exist_ok=True) # 40,000 images\n",
        "os.makedirs('images/val/class/', exist_ok=True)   #  1,000 images\n",
        "for i, file in enumerate(os.listdir('/content/paintingedit/painting')):\n",
        "  if i < 1000: # first 1000 will be val\n",
        "    os.rename('/content/paintingedit/painting/' + file, 'images/val/class/' + file)\n",
        "  else: # others will be val\n",
        "    os.rename('/content/paintingedit/painting/' + file, 'images/train/class/' + file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkBkuayZLyEt"
      },
      "source": [
        "# For plotting\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# For conversion\n",
        "from skimage.color import lab2rgb, rgb2lab, rgb2gray\n",
        "from skimage import io\n",
        "# For everything\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# For our model\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "# For utilities\n",
        "import os, shutil, time\n",
        "lab_version = 1\n",
        "from IPython.display import Image, display\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE5_oo5uEiyv",
        "outputId": "ec9fb4c1-c887-4bbe-c9e8-7926a9769801"
      },
      "source": [
        "# Check if GPU is available\n",
        "use_gpu = torch.cuda.is_available()\n",
        "print (use_gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFEt2QzwL4qS"
      },
      "source": [
        "class GrayscaleImageFolder(datasets.ImageFolder):\n",
        "  '''Custom images folder, which converts images to grayscale before loading'''\n",
        "  def __getitem__(self, index):\n",
        "    path, target = self.imgs[index]\n",
        "    img = self.loader(path)\n",
        "    if self.transform is not None:\n",
        "      img_original = self.transform(img)\n",
        "      img_original = np.asarray(img_original)\n",
        "      img_lab = rgb2lab(img_original)\n",
        "      img_lab = (img_lab + 128) / 255\n",
        "      img_ab = img_lab[:, :, 1:3]\n",
        "      img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
        "      img_original = rgb2gray(img_original)\n",
        "      img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "    return img_original, img_ab, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSRTD6ChL70y"
      },
      "source": [
        "# Training\n",
        "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()])\n",
        "train_imagefolder = GrayscaleImageFolder('images/train', train_transforms)\n",
        "train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=64, shuffle=True)\n",
        "\n",
        "# Validation \n",
        "val_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\n",
        "val_imagefolder = GrayscaleImageFolder('images/val' , val_transforms)\n",
        "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=64, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEJkTgStrHgy"
      },
      "source": [
        "# A class used for non-learnable upsampling of the images.\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "\n",
        "    def __init__(self, scale_factor=2, mode='nearest'):\n",
        "        super(Upsample, self).__init__()\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.scale_factor = scale_factor\n",
        "        self.mode = mode\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAUvkUncYvyH"
      },
      "source": [
        "**base model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mu2LkupMAjE"
      },
      "source": [
        "class ColorizationNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationNet, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,12,3,1,2),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(12,32,3,1,2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,64,3,1,2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64,32,3,1,2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32,12,3,1,2),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(12,2,3,1,2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "      \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x).relu()\n",
        "        x = self.decoder(x).relu()\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSqivmpjYy1f"
      },
      "source": [
        "**deep model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ91Hri8RJDw"
      },
      "source": [
        "class ColorizationNetdeep(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationNetdeep, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1,12,5,1,2),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(12,32,5,1,2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,32,5,1,2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,64,5,1,2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64,64,5,1,2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(64,64,5,1,2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64,32,5,1,2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,32,5,1,2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32,12,5,1,2),\n",
        "            nn.BatchNorm2d(12),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(12,2,5,1,2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x).relu()\n",
        "        x = self.decoder(x).relu()\n",
        "        return x           \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d04UrsZFY2vr"
      },
      "source": [
        "**resnet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBWJWUxYoLTL"
      },
      "source": [
        "class ColorizationResNet(nn.Module):\n",
        "  def __init__(self, input_size=128):\n",
        "    super(ColorizationResNet, self).__init__()\n",
        "    MIDLEVEL_FEATURE_SIZE = 128\n",
        "\n",
        "    ## First half: ResNet\n",
        "    resnet = models.resnet18(num_classes=365) \n",
        "    # Change first conv layer to accept single-channel (grayscale) input\n",
        "    resnet.conv1.weight = nn.Parameter(resnet.conv1.weight.sum(dim=1).unsqueeze(1)) \n",
        "    # Extract midlevel features from ResNet-gray\n",
        "    self.midlevel_resnet = nn.Sequential(*list(resnet.children())[0:6])\n",
        "\n",
        "    ## Second half: Upsampling\n",
        "    self.upsample = nn.Sequential(     \n",
        "      nn.Conv2d(MIDLEVEL_FEATURE_SIZE, 128, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.ReLU(),\n",
        "      nn.Upsample(scale_factor=2),\n",
        "      nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
        "      nn.Upsample(scale_factor=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    # Pass input through ResNet-gray to extract features\n",
        "    midlevel_features = self.midlevel_resnet(input)\n",
        "\n",
        "    # Upsample to get colors\n",
        "    output = self.upsample(midlevel_features)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XesULtHMGMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de42de28-3d26-483e-8e25-78a1ac102680"
      },
      "source": [
        "\n",
        "model = ColorizationNetdeep()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ColorizationNetdeep(\n",
            "  (encoder): Sequential(\n",
            "    (0): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(12, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU()\n",
            "    (9): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU()\n",
            "    (12): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU()\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU()\n",
            "    (9): Conv2d(32, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (10): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU()\n",
            "    (12): Conv2d(12, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (13): ReLU()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU4q5NS5MIPl"
      },
      "source": [
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hquzywxY7iK"
      },
      "source": [
        "**try different optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORakNuSxMKXw"
      },
      "source": [
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "#optimizer = torch.optim.SGD (model.parameters(), lr = learning_rate, momentum= momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ9i9LguMLeb"
      },
      "source": [
        "class AverageMeter(object):\n",
        "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "  def reset(self):\n",
        "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "  def update(self, val, n=1):\n",
        "    self.val = val\n",
        "    self.sum += val * n\n",
        "    self.count += n\n",
        "    self.avg = self.sum / self.count\n",
        "\n",
        "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
        "  '''Show/save rgb image from grayscale and ab channels\n",
        "     Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
        "  plt.clf() # clear matplotlib \n",
        "  color_image = torch.cat((grayscale_input, ab_input), 0).numpy() # combine channels\n",
        "  color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
        "  color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
        "  color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
        "  color_image = lab2rgb(color_image.astype(np.float64))\n",
        "  grayscale_input = grayscale_input.squeeze().numpy()\n",
        "  if save_path is not None and save_name is not None: \n",
        "    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
        "    plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CdZAOM4MSpc"
      },
      "source": [
        "def validate(val_loader, model, criterion, save_images, epoch):\n",
        "  model.eval()\n",
        "\n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "\n",
        "  end = time.time()\n",
        "  already_saved_images = False\n",
        "  for i, (input_gray, input_ab, target) in enumerate(val_loader):\n",
        "    data_time.update(time.time() - end)\n",
        "\n",
        "    # Use GPU\n",
        "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
        "\n",
        "    # Run model and record loss\n",
        "    output_ab = model(input_gray) # throw away class predictions\n",
        "    loss = criterion(output_ab, input_ab)\n",
        "    losses.update(loss.item(), input_gray.size(0))\n",
        "\n",
        "    # Save images to file\n",
        "    if save_images and not already_saved_images:\n",
        "      already_saved_images = True\n",
        "      for j in range(min(len(output_ab), 10)): # save at most 5 images\n",
        "        save_path = {'grayscale': '/content/drive/MyDrive/outputsRMdeep/gray', 'colorized': '/content/drive/MyDrive/outputsRMdeep/color'}\n",
        "        save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
        "        to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n",
        "        #display(Image('outputs/color/'+save_name))\n",
        "    # Record time to do forward passes and save images\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy -- in the code below, val refers to both value and validation\n",
        "    if i % 25 == 0:\n",
        "      print('Validate: [{0}/{1}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "             i, len(val_loader), batch_time=batch_time, loss=losses))\n",
        "\n",
        "  print('Finished validation.')\n",
        "  return losses.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV8mParhMXCW"
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "  print('Starting training epoch {}'.format(epoch))\n",
        "  model.train()\n",
        "  \n",
        "  # Prepare value counters and timers\n",
        "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
        "\n",
        "  end = time.time()\n",
        "  for i, (input_gray, input_ab, target) in enumerate(train_loader):\n",
        "    \n",
        "    # Use GPU if available\n",
        "    if use_gpu: input_gray, input_ab, target = input_gray.cuda(), input_ab.cuda(), target.cuda()\n",
        "\n",
        "    # Record time to load data (above)\n",
        "    data_time.update(time.time() - end)\n",
        "\n",
        "    # Run forward pass\n",
        "    output_ab = model(input_gray) \n",
        "    loss = criterion(output_ab, input_ab) \n",
        "    losses.update(loss.item(), input_gray.size(0))\n",
        "\n",
        "    # Compute gradient and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Record time to do forward and backward passes\n",
        "    batch_time.update(time.time() - end)\n",
        "    end = time.time()\n",
        "\n",
        "    # Print model accuracy -- in the code below, val refers to value, not validation\n",
        "    if i % 25 == 0:\n",
        "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "              epoch, i, len(train_loader), batch_time=batch_time,\n",
        "             data_time=data_time, loss=losses)) \n",
        "\n",
        "  print('Finished training epoch {}'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tofnQOkpRKY"
      },
      "source": [
        "# Move model and loss function to GPU\n",
        "if use_gpu: \n",
        "  criterion = criterion.cuda()\n",
        "  model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LJNMwxkMaOe"
      },
      "source": [
        "# Make folders and set parameters\n",
        "os.makedirs('/content/drive/MyDrive/outputsRMdeep/color', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/outputsRMdeep/gray', exist_ok=True)\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "save_images = True\n",
        "best_losses = 1e10\n",
        "epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGzNxJgqZQX4"
      },
      "source": [
        "# Train model\n",
        "for epoch in range(epochs):\n",
        "  # Train for one epoch, then validate\n",
        "  train(train_loader, model, criterion, optimizer, epoch)\n",
        "  with torch.no_grad():\n",
        "    losses = validate(val_loader, model, criterion, save_images, epoch)\n",
        "  # Save checkpoint and replace old best model if current model is better\n",
        "  if losses < best_losses:\n",
        "    best_losses = losses\n",
        "    torch.save(model.state_dict(), 'checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}