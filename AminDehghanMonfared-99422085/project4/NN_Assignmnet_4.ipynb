{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NN_Assignmnet_4.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOXbmArhoBdtQ5A6fs1yXIA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8a1638675f23465cba1f8aec2c08f263":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0bc59874be2c4dd98ebe54fb51897b58","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_40e61ff8a63f44f5aaf590fbc06c775f","IPY_MODEL_2e48ec49a30441cbad04693b7284bb81"]}},"0bc59874be2c4dd98ebe54fb51897b58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"40e61ff8a63f44f5aaf590fbc06c775f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_73cfc9a5511a41eabecfd0c0c858db62","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a64509fcaab49fcaf2dc4748afba5a1"}},"2e48ec49a30441cbad04693b7284bb81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dcfadb8dd48d40fbb75cd0b24454a54e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:02&lt;00:00, 109kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_16d4ff792cd847eb83aa79087247e4d9"}},"73cfc9a5511a41eabecfd0c0c858db62":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7a64509fcaab49fcaf2dc4748afba5a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dcfadb8dd48d40fbb75cd0b24454a54e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"16d4ff792cd847eb83aa79087247e4d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66db64fdeb104aae975d05e332809a83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_439a13186bb3484a86697cc9c0ca3acf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_de268f38d646408286d3d212b07fcff5","IPY_MODEL_18d39178a55945868f3be088ca996e9e"]}},"439a13186bb3484a86697cc9c0ca3acf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de268f38d646408286d3d212b07fcff5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7722741f4df449c6863099a1ceff6048","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b00882c2be314f90a5573038ba611d58"}},"18d39178a55945868f3be088ca996e9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5df7470d21f742be9bd94442196a510c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 34.4B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c09c95425c79413a959b48062d12e90b"}},"7722741f4df449c6863099a1ceff6048":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b00882c2be314f90a5573038ba611d58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5df7470d21f742be9bd94442196a510c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c09c95425c79413a959b48062d12e90b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8fb869b3f3f54e8d951742cf0d8dc4c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d09a56b774644c21aa069ee218ce71d3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_97f034f95b2b450b9278d4716084dd62","IPY_MODEL_e16f4778ceda4faca18ce170685cd37e"]}},"d09a56b774644c21aa069ee218ce71d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"97f034f95b2b450b9278d4716084dd62":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c5b6e4dccb1c4ac0aaace30a13830cd1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9590236ea9c64b6ba80abc61deade33e"}},"e16f4778ceda4faca18ce170685cd37e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1a9147fca45b4bc9afbcf88d5e574b87","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 2.27MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a987fee8ebb403ab9324e9b0ae3585b"}},"c5b6e4dccb1c4ac0aaace30a13830cd1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9590236ea9c64b6ba80abc61deade33e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1a9147fca45b4bc9afbcf88d5e574b87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6a987fee8ebb403ab9324e9b0ae3585b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"5Sr0_-J9iVZJ"},"source":["# **Neural Network Exercise 4**\n","## Part 1\n","\n","---\n","\n","Teacher : Dr.Kheradpisheh \n","\n","Student : Amin Dehghan Monfared\t     \n","SID : 99422085 \n","\n","\n","\n","Spring 2021 "]},{"cell_type":"code","metadata":{"id":"TqpH0L2OiQYz","executionInfo":{"status":"ok","timestamp":1625997012236,"user_tz":-270,"elapsed":5029,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["import os\n","import sys\n","import random\n","import time\n","import datetime\n","import shutil\n","import glob\n","\n","import numpy as np \n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import time\n","import datetime\n","\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.utils import make_grid\n","from torchvision.datasets import ImageFolder\n","from torch.autograd import Variable\n","\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import functools\n","import operator\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1r6bzzbJzPRv"},"source":["# Get Data"]},{"cell_type":"markdown","metadata":{"id":"ul0O_6wiDmf9"},"source":["https://www.kaggle.com/c/nlp-getting-started"]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":385},"id":"NP3FFjvIvc3a","executionInfo":{"status":"ok","timestamp":1625997037780,"user_tz":-270,"elapsed":25555,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"ef937dc7-e5f6-49c9-e92c-cf28b77bcfd2"},"source":["# Download Data From Kaggle api\n","\n","!pip install -q kaggle\n","\n","!pip install --upgrade --force-reinstall --no-deps kaggle\n","\n","from google.colab import files\n","files.upload()\n","\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","!kaggle competitions download -c nlp-getting-started\n","\n","\n","# unzip files \n","import zipfile \n","\n","zip_ref = zipfile.ZipFile('nlp-getting-started.zip','r')\n","zip_ref.extractall('./data/')\n","zip_ref.close()\n","\n","!rm ./nlp-getting-started.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting kaggle\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e7/3bac01547d2ed3d308ac92a0878fbdb0ed0f3d41fb1906c319ccbba1bfbc/kaggle-1.5.12.tar.gz (58kB)\n","\r\u001b[K     |█████▋                          | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 19.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 17.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.12-cp37-none-any.whl size=73053 sha256=3c0bed6719bbe3754b1e33b482537208ee4b15eab86926ffb2a7dccc52891d4c\n","  Stored in directory: /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.12\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-3a63ec8e-3083-4d4e-83e3-5c2c7fdb390a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3a63ec8e-3083-4d4e-83e3-5c2c7fdb390a\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving kaggle.json to kaggle.json\n","Downloading nlp-getting-started.zip to /content\n","  0% 0.00/593k [00:00<?, ?B/s]\n","100% 593k/593k [00:00<00:00, 112MB/s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ux_2i1R9BIaL"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"B8wV8cHuLYIr","executionInfo":{"status":"ok","timestamp":1625997043709,"user_tz":-270,"elapsed":316,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["train_df = pd.read_csv('./data/train.csv')\n","test_df = pd.read_csv('./data/test.csv')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ddE3dJBuLjfj","executionInfo":{"status":"ok","timestamp":1625997044028,"user_tz":-270,"elapsed":17,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"6544b4c1-b56f-46c4-ca94-f3d005d6edf7"},"source":["train_df.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ...                                               text target\n","0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n","1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n","2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n","3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n","4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZwPvXvqLmAU","executionInfo":{"status":"ok","timestamp":1625997046695,"user_tz":-270,"elapsed":352,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"696fc4be-b7f9-4377-fad0-f5f72dca4564"},"source":["train_df.info()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7613 entries, 0 to 7612\n","Data columns (total 5 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   id        7613 non-null   int64 \n"," 1   keyword   7552 non-null   object\n"," 2   location  5080 non-null   object\n"," 3   text      7613 non-null   object\n"," 4   target    7613 non-null   int64 \n","dtypes: int64(2), object(3)\n","memory usage: 297.5+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eeTXvWZyMCaB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625997049720,"user_tz":-270,"elapsed":827,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"bb7365e0-0443-439d-a8ad-73b2dc4bb7d1"},"source":["%time\n","import os\n","import sys\n","import warnings\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","    \n","import numpy as np\n","import pandas as pd\n","import sklearn\n","\n","# Libraries and packages for text (pre-)processing \n","import string\n","import re\n","import nltk\n","\n","print(\"Python version:\", sys.version)\n","print(\"Version info.:\", sys.version_info)\n","print(\"pandas version:\", pd.__version__)\n","print(\"numpy version:\", np.__version__)\n","print(\"skearn version:\", sklearn.__version__)\n","print(\"re version:\", re.__version__)\n","print(\"nltk version:\", nltk.__version__)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n","Wall time: 5.72 µs\n","Python version: 3.7.10 (default, May  3 2021, 02:48:31) \n","[GCC 7.5.0]\n","Version info.: sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\n","pandas version: 1.1.5\n","numpy version: 1.19.5\n","skearn version: 0.22.2.post1\n","re version: 2.2.1\n","nltk version: 3.2.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4QgLD8-tEFD_","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1625912092742,"user_tz":-270,"elapsed":449,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"3c5dbb4b-2e3a-4250-d76d-e5033200405a"},"source":["train_df[\"text_clean\"] = train_df[\"text\"].apply(lambda x: x.lower())\n","display(train_df.head())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ... target                                         text_clean\n","0   1     NaN  ...      1  our deeds are the reason of this #earthquake m...\n","1   4     NaN  ...      1             forest fire near la ronge sask. canada\n","2   5     NaN  ...      1  all residents asked to 'shelter in place' are ...\n","3   6     NaN  ...      1  13,000 people receive #wildfires evacuation or...\n","4   7     NaN  ...      1  just got sent this photo from ruby #alaska as ...\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fDM8OeMZVqcD","executionInfo":{"status":"ok","timestamp":1625912115626,"user_tz":-270,"elapsed":6557,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"9b99496c-f560-4188-aa71-3697952077af"},"source":["!pip install contractions"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting contractions\n","  Downloading https://files.pythonhosted.org/packages/93/f4/0ec4a458e4368cc3be2c799411ecf0bc961930e566dadb9624563821b3a6/contractions-0.0.52-py2.py3-none-any.whl\n","Collecting textsearch>=0.0.21\n","  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n","Collecting anyascii\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n","\u001b[K     |████████████████████████████████| 286kB 3.9MB/s \n","\u001b[?25hCollecting pyahocorasick\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n","\u001b[K     |████████████████████████████████| 327kB 28.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n","  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85406 sha256=851b6fa78c9d4218d1b7464fdade0b09cb4dc66593b87ec2c9c61d507c04c872\n","  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n","Successfully built pyahocorasick\n","Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n","Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmT_e0S7VrA_","executionInfo":{"status":"ok","timestamp":1625912124360,"user_tz":-270,"elapsed":381,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"e0ed04bc-e5bd-4388-9b69-7ead7a0ec88a"},"source":["%time\n","import contractions\n","\n","# Test\n","test_text = \"\"\"\n","            Y'all can't expand contractions I'd think. I'd like to know how I'd done that! \n","            We're going to the zoo and I don't think I'll be home for dinner.\n","            Theyre going to the zoo and she'll be home for dinner.\n","            We should've do it in here but we shouldn't've eat it\n","            \"\"\"\n","print(\"Test: \", contractions.fix(test_text))\n","\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: contractions.fix(x))\n","\n","# double check\n","print(train_df[\"text\"][67])\n","print(train_df[\"text_clean\"][67])\n","print(train_df[\"text\"][12])\n","print(train_df[\"text_clean\"][12])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n","Wall time: 6.2 µs\n","Test:  \n","            you all cannot expand contractions I would think. I would like to know how I would done that! \n","            we are going to the zoo and I do not think I will be home for dinner.\n","            they are going to the zoo and she will be home for dinner.\n","            We should have do it in here but we should not have eat it\n","            \n","'I can't have kids cuz I got in a bicycle accident &amp; split my testicles. it's impossible for me to have kids' MICHAEL YOU ARE THE FATHER\n","'i cannot have kids cuz i got in a bicycle accident &amp; split my testicles. it is impossible for me to have kids' michael you are the father\n","#raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count \n","#raining #flooding #florida #tampabay #tampa 18 or 19 days. I have lost count \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rn9W1KkdVvYG"},"source":["def remove_URL(text):\n","    \"\"\"\n","        Remove URLs from a sample string\n","    \"\"\"\n","    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcDCk6TVVzjl","executionInfo":{"status":"ok","timestamp":1625912216461,"user_tz":-270,"elapsed":25,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"79960113-1c85-4915-e31f-ceb2b01a1561"},"source":["# remove urls from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))\n","\n","# double check\n","print(train_df[\"text\"][31])\n","print(train_df[\"text_clean\"][31])\n","print(train_df[\"text\"][37])\n","print(train_df[\"text_clean\"][37])\n","print(train_df[\"text\"][62])\n","print(train_df[\"text_clean\"][62])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n","@bbcmtd wholesale markets ablaze \n","INEC Office in Abia Set Ablaze - http://t.co/3ImaomknnA\n","inec office in abia set ablaze - \n","Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n","rene ablaze &amp; jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YW1bha0LV1SX"},"source":["def remove_html(text):\n","    \"\"\"\n","        Remove the html in sample text\n","    \"\"\"\n","    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n","    return re.sub(html, \"\", text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UC9f9iXnV2_P","executionInfo":{"status":"ok","timestamp":1625912223825,"user_tz":-270,"elapsed":6,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"e83ca32f-8e4a-466b-f2e7-ac6b288b5fe5"},"source":["\n","\n","# remove html from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))\n","\n","# double check\n","print(train_df[\"text\"][62])\n","print(train_df[\"text_clean\"][62])\n","print(train_df[\"text\"][7385])\n","print(train_df[\"text_clean\"][7385])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n","rene ablaze  jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n","NW Michigan #WindStorm (Sheer) Recovery Updates: Leelanau &amp; Grand Traverse - State of Emergency 2b extended http://t.co/OSKfyj8CK7 #BeSafe\n","nw michigan #windstorm (sheer) recovery updates: leelanau  grand traverse - state of emergency 2b extended  #besafe\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ADYxdodsV4tk"},"source":["def remove_non_ascii(text):\n","    \"\"\"\n","        Remove non-ASCII characters \n","    \"\"\"\n","    return re.sub(r'[^\\x00-\\x7f]',r'', text) # or ''.join([x for x in text if x in string.printable]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2YnmmGbdV6Qa","executionInfo":{"status":"ok","timestamp":1625912230948,"user_tz":-270,"elapsed":6,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"7c61c2be-5a9b-4315-d01c-7073cec88b15"},"source":["# remove non-ascii characters from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))\n","\n","# double check\n","print(train_df[\"text\"][38])\n","print(train_df[\"text_clean\"][38])\n","print(train_df[\"text\"][7586])\n","print(train_df[\"text_clean\"][7586])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J\n","barbados #bridgetown jamaica  two cars set ablaze: santa cruz  head of the st elizabeth police superintende...  \n","#Sismo DETECTADO #JapÌ_n 15:41:07 Seismic intensity 0 Iwate Miyagi JST #?? http://t.co/gMoUl9zQ2Q\n","#sismo detectado #jap_n 15:41:07 seismic intensity 0 iwate miyagi jst #?? \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8rwJfaF0V9NI"},"source":["def remove_special_characters(text):\n","    \"\"\"\n","        Remove special special characters, including symbols, emojis, and other graphic characters\n","    \"\"\"\n","    emoji_pattern = re.compile(\n","        '['\n","        u'\\U0001F600-\\U0001F64F'  # emoticons\n","        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n","        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n","        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n","        u'\\U00002702-\\U000027B0'\n","        u'\\U000024C2-\\U0001F251'\n","        ']+',\n","        flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"gpyqMrHhV-2m","executionInfo":{"status":"ok","timestamp":1625912357781,"user_tz":-270,"elapsed":408,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"5c717b93-4ccc-4286-f5f6-e72c00421180"},"source":["%time\n","# remove non-ascii characters from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_special_characters(x))\n","display(train_df.head())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n","Wall time: 18.1 µs\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ... target                                         text_clean\n","0   1     NaN  ...      1  our deeds are the reason of this #earthquake m...\n","1   4     NaN  ...      1             forest fire near la ronge sask. canada\n","2   5     NaN  ...      1  all residents asked to 'shelter in place' are ...\n","3   6     NaN  ...      1  13,000 people receive #wildfires evacuation or...\n","4   7     NaN  ...      1  just got sent this photo from ruby #alaska as ...\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"0j0TRczHWCY_"},"source":["def remove_punct(text):\n","    \"\"\"\n","        Remove the punctuation\n","    \"\"\"\n","#     return re.sub(r'[]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~-]+', \"\", text)\n","    return text.translate(str.maketrans('', '', string.punctuation))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"FnwYyqjMWNRO","executionInfo":{"status":"ok","timestamp":1625912391255,"user_tz":-270,"elapsed":22,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"20fa900a-4c99-45cf-87f0-79d549e0d62f"},"source":["%time\n","# remove non-ascii characters from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_special_characters(x))\n","display(train_df.head())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n","Wall time: 8.34 µs\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ... target                                         text_clean\n","0   1     NaN  ...      1  our deeds are the reason of this #earthquake m...\n","1   4     NaN  ...      1             forest fire near la ronge sask. canada\n","2   5     NaN  ...      1  all residents asked to 'shelter in place' are ...\n","3   6     NaN  ...      1  13,000 people receive #wildfires evacuation or...\n","4   7     NaN  ...      1  just got sent this photo from ruby #alaska as ...\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"bGZ-C80uWYs1"},"source":["def other_clean(text):\n","        \"\"\"\n","            Other manual text cleaning techniques\n","        \"\"\"\n","        # Typos, slang and other\n","        sample_typos_slang = {\n","                                \"w/e\": \"whatever\",\n","                                \"usagov\": \"usa government\",\n","                                \"recentlu\": \"recently\",\n","                                \"ph0tos\": \"photos\",\n","                                \"amirite\": \"am i right\",\n","                                \"exp0sed\": \"exposed\",\n","                                \"<3\": \"love\",\n","                                \"luv\": \"love\",\n","                                \"amageddon\": \"armageddon\",\n","                                \"trfc\": \"traffic\",\n","                                \"16yr\": \"16 year\"\n","                                }\n","\n","        # Acronyms\n","        sample_acronyms =  { \n","                            \"mh370\": \"malaysia airlines flight 370\",\n","                            \"okwx\": \"oklahoma city weather\",\n","                            \"arwx\": \"arkansas weather\",    \n","                            \"gawx\": \"georgia weather\",  \n","                            \"scwx\": \"south carolina weather\",  \n","                            \"cawx\": \"california weather\",\n","                            \"tnwx\": \"tennessee weather\",\n","                            \"azwx\": \"arizona weather\",  \n","                            \"alwx\": \"alabama weather\",\n","                            \"usnwsgov\": \"united states national weather service\",\n","                            \"2mw\": \"tomorrow\"\n","                            }\n","\n","        \n","        # Some common abbreviations \n","        sample_abbr = {\n","                        \"$\" : \" dollar \",\n","                        \"€\" : \" euro \",\n","                        \"4ao\" : \"for adults only\",\n","                        \"a.m\" : \"before midday\",\n","                        \"a3\" : \"anytime anywhere anyplace\",\n","                        \"aamof\" : \"as a matter of fact\",\n","                        \"acct\" : \"account\",\n","                        \"adih\" : \"another day in hell\",\n","                        \"afaic\" : \"as far as i am concerned\",\n","                        \"afaict\" : \"as far as i can tell\",\n","                        \"afaik\" : \"as far as i know\",\n","                        \"afair\" : \"as far as i remember\",\n","                        \"afk\" : \"away from keyboard\",\n","                        \"app\" : \"application\",\n","                        \"approx\" : \"approximately\",\n","                        \"apps\" : \"applications\",\n","                        \"asap\" : \"as soon as possible\",\n","                        \"asl\" : \"age, sex, location\",\n","                        \"atk\" : \"at the keyboard\",\n","                        \"ave.\" : \"avenue\",\n","                        \"aymm\" : \"are you my mother\",\n","                        \"ayor\" : \"at your own risk\", \n","                        \"b&b\" : \"bed and breakfast\",\n","                        \"b+b\" : \"bed and breakfast\",\n","                        \"b.c\" : \"before christ\",\n","                        \"b2b\" : \"business to business\",\n","                        \"b2c\" : \"business to customer\",\n","                        \"b4\" : \"before\",\n","                        \"b4n\" : \"bye for now\",\n","                        \"b@u\" : \"back at you\",\n","                        \"bae\" : \"before anyone else\",\n","                        \"bak\" : \"back at keyboard\",\n","                        \"bbbg\" : \"bye bye be good\",\n","                        \"bbc\" : \"british broadcasting corporation\",\n","                        \"bbias\" : \"be back in a second\",\n","                        \"bbl\" : \"be back later\",\n","                        \"bbs\" : \"be back soon\",\n","                        \"be4\" : \"before\",\n","                        \"bfn\" : \"bye for now\",\n","                        \"blvd\" : \"boulevard\",\n","                        \"bout\" : \"about\",\n","                        \"brb\" : \"be right back\",\n","                        \"bros\" : \"brothers\",\n","                        \"brt\" : \"be right there\",\n","                        \"bsaaw\" : \"big smile and a wink\",\n","                        \"btw\" : \"by the way\",\n","                        \"bwl\" : \"bursting with laughter\",\n","                        \"c/o\" : \"care of\",\n","                        \"cet\" : \"central european time\",\n","                        \"cf\" : \"compare\",\n","                        \"cia\" : \"central intelligence agency\",\n","                        \"csl\" : \"can not stop laughing\",\n","                        \"cu\" : \"see you\",\n","                        \"cul8r\" : \"see you later\",\n","                        \"cv\" : \"curriculum vitae\",\n","                        \"cwot\" : \"complete waste of time\",\n","                        \"cya\" : \"see you\",\n","                        \"cyt\" : \"see you tomorrow\",\n","                        \"dae\" : \"does anyone else\",\n","                        \"dbmib\" : \"do not bother me i am busy\",\n","                        \"diy\" : \"do it yourself\",\n","                        \"dm\" : \"direct message\",\n","                        \"dwh\" : \"during work hours\",\n","                        \"e123\" : \"easy as one two three\",\n","                        \"eet\" : \"eastern european time\",\n","                        \"eg\" : \"example\",\n","                        \"embm\" : \"early morning business meeting\",\n","                        \"encl\" : \"enclosed\",\n","                        \"encl.\" : \"enclosed\",\n","                        \"etc\" : \"and so on\",\n","                        \"faq\" : \"frequently asked questions\",\n","                        \"fawc\" : \"for anyone who cares\",\n","                        \"fb\" : \"facebook\",\n","                        \"fc\" : \"fingers crossed\",\n","                        \"fig\" : \"figure\",\n","                        \"fimh\" : \"forever in my heart\", \n","                        \"ft.\" : \"feet\",\n","                        \"ft\" : \"featuring\",\n","                        \"ftl\" : \"for the loss\",\n","                        \"ftw\" : \"for the win\",\n","                        \"fwiw\" : \"for what it is worth\",\n","                        \"fyi\" : \"for your information\",\n","                        \"g9\" : \"genius\",\n","                        \"gahoy\" : \"get a hold of yourself\",\n","                        \"gal\" : \"get a life\",\n","                        \"gcse\" : \"general certificate of secondary education\",\n","                        \"gfn\" : \"gone for now\",\n","                        \"gg\" : \"good game\",\n","                        \"gl\" : \"good luck\",\n","                        \"glhf\" : \"good luck have fun\",\n","                        \"gmt\" : \"greenwich mean time\",\n","                        \"gmta\" : \"great minds think alike\",\n","                        \"gn\" : \"good night\",\n","                        \"g.o.a.t\" : \"greatest of all time\",\n","                        \"goat\" : \"greatest of all time\",\n","                        \"goi\" : \"get over it\",\n","                        \"gps\" : \"global positioning system\",\n","                        \"gr8\" : \"great\",\n","                        \"gratz\" : \"congratulations\",\n","                        \"gyal\" : \"girl\",\n","                        \"h&c\" : \"hot and cold\",\n","                        \"hp\" : \"horsepower\",\n","                        \"hr\" : \"hour\",\n","                        \"hrh\" : \"his royal highness\",\n","                        \"ht\" : \"height\",\n","                        \"ibrb\" : \"i will be right back\",\n","                        \"ic\" : \"i see\",\n","                        \"icq\" : \"i seek you\",\n","                        \"icymi\" : \"in case you missed it\",\n","                        \"idc\" : \"i do not care\",\n","                        \"idgadf\" : \"i do not give a damn fuck\",\n","                        \"idgaf\" : \"i do not give a fuck\",\n","                        \"idk\" : \"i do not know\",\n","                        \"ie\" : \"that is\",\n","                        \"i.e\" : \"that is\",\n","                        \"ifyp\" : \"i feel your pain\",\n","                        \"IG\" : \"instagram\",\n","                        \"iirc\" : \"if i remember correctly\",\n","                        \"ilu\" : \"i love you\",\n","                        \"ily\" : \"i love you\",\n","                        \"imho\" : \"in my humble opinion\",\n","                        \"imo\" : \"in my opinion\",\n","                        \"imu\" : \"i miss you\",\n","                        \"iow\" : \"in other words\",\n","                        \"irl\" : \"in real life\",\n","                        \"j4f\" : \"just for fun\",\n","                        \"jic\" : \"just in case\",\n","                        \"jk\" : \"just kidding\",\n","                        \"jsyk\" : \"just so you know\",\n","                        \"l8r\" : \"later\",\n","                        \"lb\" : \"pound\",\n","                        \"lbs\" : \"pounds\",\n","                        \"ldr\" : \"long distance relationship\",\n","                        \"lmao\" : \"laugh my ass off\",\n","                        \"lmfao\" : \"laugh my fucking ass off\",\n","                        \"lol\" : \"laughing out loud\",\n","                        \"ltd\" : \"limited\",\n","                        \"ltns\" : \"long time no see\",\n","                        \"m8\" : \"mate\",\n","                        \"mf\" : \"motherfucker\",\n","                        \"mfs\" : \"motherfuckers\",\n","                        \"mfw\" : \"my face when\",\n","                        \"mofo\" : \"motherfucker\",\n","                        \"mph\" : \"miles per hour\",\n","                        \"mr\" : \"mister\",\n","                        \"mrw\" : \"my reaction when\",\n","                        \"ms\" : \"miss\",\n","                        \"mte\" : \"my thoughts exactly\",\n","                        \"nagi\" : \"not a good idea\",\n","                        \"nbc\" : \"national broadcasting company\",\n","                        \"nbd\" : \"not big deal\",\n","                        \"nfs\" : \"not for sale\",\n","                        \"ngl\" : \"not going to lie\",\n","                        \"nhs\" : \"national health service\",\n","                        \"nrn\" : \"no reply necessary\",\n","                        \"nsfl\" : \"not safe for life\",\n","                        \"nsfw\" : \"not safe for work\",\n","                        \"nth\" : \"nice to have\",\n","                        \"nvr\" : \"never\",\n","                        \"nyc\" : \"new york city\",\n","                        \"oc\" : \"original content\",\n","                        \"og\" : \"original\",\n","                        \"ohp\" : \"overhead projector\",\n","                        \"oic\" : \"oh i see\",\n","                        \"omdb\" : \"over my dead body\",\n","                        \"omg\" : \"oh my god\",\n","                        \"omw\" : \"on my way\",\n","                        \"p.a\" : \"per annum\",\n","                        \"p.m\" : \"after midday\",\n","                        \"pm\" : \"prime minister\",\n","                        \"poc\" : \"people of color\",\n","                        \"pov\" : \"point of view\",\n","                        \"pp\" : \"pages\",\n","                        \"ppl\" : \"people\",\n","                        \"prw\" : \"parents are watching\",\n","                        \"ps\" : \"postscript\",\n","                        \"pt\" : \"point\",\n","                        \"ptb\" : \"please text back\",\n","                        \"pto\" : \"please turn over\",\n","                        \"qpsa\" : \"what happens\", #\"que pasa\",\n","                        \"ratchet\" : \"rude\",\n","                        \"rbtl\" : \"read between the lines\",\n","                        \"rlrt\" : \"real life retweet\", \n","                        \"rofl\" : \"rolling on the floor laughing\",\n","                        \"roflol\" : \"rolling on the floor laughing out loud\",\n","                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","                        \"rt\" : \"retweet\",\n","                        \"ruok\" : \"are you ok\",\n","                        \"sfw\" : \"safe for work\",\n","                        \"sk8\" : \"skate\",\n","                        \"smh\" : \"shake my head\",\n","                        \"sq\" : \"square\",\n","                        \"srsly\" : \"seriously\", \n","                        \"ssdd\" : \"same stuff different day\",\n","                        \"tbh\" : \"to be honest\",\n","                        \"tbs\" : \"tablespooful\",\n","                        \"tbsp\" : \"tablespooful\",\n","                        \"tfw\" : \"that feeling when\",\n","                        \"thks\" : \"thank you\",\n","                        \"tho\" : \"though\",\n","                        \"thx\" : \"thank you\",\n","                        \"tia\" : \"thanks in advance\",\n","                        \"til\" : \"today i learned\",\n","                        \"tl;dr\" : \"too long i did not read\",\n","                        \"tldr\" : \"too long i did not read\",\n","                        \"tmb\" : \"tweet me back\",\n","                        \"tntl\" : \"trying not to laugh\",\n","                        \"ttyl\" : \"talk to you later\",\n","                        \"u\" : \"you\",\n","                        \"u2\" : \"you too\",\n","                        \"u4e\" : \"yours for ever\",\n","                        \"utc\" : \"coordinated universal time\",\n","                        \"w/\" : \"with\",\n","                        \"w/o\" : \"without\",\n","                        \"w8\" : \"wait\",\n","                        \"wassup\" : \"what is up\",\n","                        \"wb\" : \"welcome back\",\n","                        \"wtf\" : \"what the fuck\",\n","                        \"wtg\" : \"way to go\",\n","                        \"wtpa\" : \"where the party at\",\n","                        \"wuf\" : \"where are you from\",\n","                        \"wuzup\" : \"what is up\",\n","                        \"wywh\" : \"wish you were here\",\n","                        \"yd\" : \"yard\",\n","                        \"ygtr\" : \"you got that right\",\n","                        \"ynk\" : \"you never know\",\n","                        \"zzz\" : \"sleeping bored and tired\"\n","                        }\n","            \n","        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n","        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n","        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n","        \n","        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n","        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n","        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n","        \n","        return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUrrwNnjWcEF","executionInfo":{"status":"ok","timestamp":1625912425497,"user_tz":-270,"elapsed":1874,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"f65e57a3-0b9f-4bfa-c67d-7baa98f406a0"},"source":["%time\n","\n","# Test\n","test_text = \"\"\"\n","            brb with some sample ph0tos I lov u. I need some $ for 2mw.\n","            \"\"\"\n","print(\"Test: \", other_clean(test_text))\n","\n","# remove punctuations from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: other_clean(x))\n","\n","# double check\n","print(train_df[\"text\"][1844])\n","print(train_df[\"text_clean\"][1844])\n","print(train_df[\"text\"][4409])\n","print(train_df[\"text_clean\"][4409])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n","Wall time: 6.44 µs\n","Test:  \n","            be right back with some sample photos I lov you. I need some  dollar  for tomorrow.\n","            \n","MH370: Intact part lifts odds plane glided not crashed into sea http://t.co/8pdnHH6tzH\n","malaysia airlines flight 370: intact part lifts odds plane glided not crashed into sea \n","@USAgov Koreans are performing hijacking of the Tokyo Olympic Games.https://t.co/APkSnpLXZj\n","@usa government koreans are performing hijacking of the tokyo olympic games.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"-K7h0kmPWdy8","executionInfo":{"status":"error","timestamp":1625913640066,"user_tz":-270,"elapsed":466619,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"934c430d-3bff-4f84-ae99-0631c524fa55"},"source":["# $$ Be cautious it takes a lot of time $$\n","from textblob import TextBlob\n","print(\"Test: \", TextBlob(\"sleapy and tehre is no plaxe I'm gioong to.\").correct())\n","\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: TextBlob(x).correct())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test:  sleepy and there is no place I'm going to.\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-f6f94bb74dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sleapy and tehre is no plaxe I'm gioong to.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_clean\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-23-f6f94bb74dd1>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sleapy and tehre is no plaxe I'm gioong to.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_clean\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mcorrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# regex matches: word or punctuation or whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mcorrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         '''\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspellcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mspellcheck\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         '''\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/en/__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\n\u001b[1;32m    122\u001b[0m     \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m                   \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m                   \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_edit2\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__contains__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__getitem__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/textblob/_text.py\u001b[0m in \u001b[0;36m_lazy\u001b[0;34m(self, method, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mReplaces\u001b[0m \u001b[0mlazydict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcalls\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"-FxfFIC_-3GB"},"source":["# Tokenize "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWuFw3aDAmCZ","executionInfo":{"status":"ok","timestamp":1625913718024,"user_tz":-270,"elapsed":1117,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"2aa0c1d0-6c35-4070-e6a1-9e1a27ec5132"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"PoUn_w_BWfJX","executionInfo":{"status":"ok","timestamp":1625913756434,"user_tz":-270,"elapsed":1582,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"e5061019-3fad-44f6-f2a1-a5e6dc97076a"},"source":["# Tokenizing the tweet base texts.\n","from nltk.tokenize import word_tokenize\n","\n","train_df['tokenized'] = train_df['text_clean'].apply(word_tokenize)\n","train_df.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","      <td>[our, deeds, are, the, reason, of, this, #, ea...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","      <td>[all, residents, asked, to, 'shelter, in, plac...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, #, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                          tokenized\n","0   1  ...  [our, deeds, are, the, reason, of, this, #, ea...\n","1   4  ...   [forest, fire, near, la, ronge, sask, ., canada]\n","2   5  ...  [all, residents, asked, to, 'shelter, in, plac...\n","3   6  ...  [13,000, people, receive, #, wildfires, evacua...\n","4   7  ...  [just, got, sent, this, photo, from, ruby, #, ...\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"id":"bWV8ndhQWhGQ","executionInfo":{"status":"ok","timestamp":1625913772773,"user_tz":-270,"elapsed":297,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"4466b8fb-04eb-43d2-fc6b-a8b9bd07864b"},"source":["\n","\n","# Removing stopwords.\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords\n","\n","stop = set(stopwords.words('english'))\n","train_df['stopwords_removed'] = train_df['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n","train_df.head()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","      <th>stopwords_removed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","      <td>[our, deeds, are, the, reason, of, this, #, ea...</td>\n","      <td>[deeds, reason, #, earthquake, may, allah, for...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","      <td>[all, residents, asked, to, 'shelter, in, plac...</td>\n","      <td>[residents, asked, 'shelter, place, ', notifie...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, #, ...</td>\n","      <td>[got, sent, photo, ruby, #, alaska, smoke, #, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                  stopwords_removed\n","0   1  ...  [deeds, reason, #, earthquake, may, allah, for...\n","1   4  ...   [forest, fire, near, la, ronge, sask, ., canada]\n","2   5  ...  [residents, asked, 'shelter, place, ', notifie...\n","3   6  ...  [13,000, people, receive, #, wildfires, evacua...\n","4   7  ...  [got, sent, photo, ruby, #, alaska, smoke, #, ...\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"-0uCa9BYWjED"},"source":["from nltk.stem import PorterStemmer\n","\n","def porter_stemmer(text):\n","    \"\"\"\n","        Stem words in list of tokenized words with PorterStemmer\n","    \"\"\"\n","    stemmer = nltk.PorterStemmer()\n","    stems = [stemmer.stem(i) for i in text]\n","    return stems"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"id":"50-s6lxFWkXY","executionInfo":{"status":"ok","timestamp":1625913784614,"user_tz":-270,"elapsed":1785,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"6c213226-1f96-4b7f-e505-078ea48bde46"},"source":["%time \n","\n","train_df['porter_stemmer'] = train_df['stopwords_removed'].apply(lambda x: porter_stemmer(x))\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n","Wall time: 6.2 µs\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","      <th>stopwords_removed</th>\n","      <th>porter_stemmer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","      <td>[our, deeds, are, the, reason, of, this, #, ea...</td>\n","      <td>[deeds, reason, #, earthquake, may, allah, for...</td>\n","      <td>[deed, reason, #, earthquak, may, allah, forgi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, rong, sask, ., canada]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","      <td>[all, residents, asked, to, 'shelter, in, plac...</td>\n","      <td>[residents, asked, 'shelter, place, ', notifie...</td>\n","      <td>[resid, ask, 'shelter, place, ', notifi, offic...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","      <td>[13,000, peopl, receiv, #, wildfir, evacu, ord...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, #, ...</td>\n","      <td>[got, sent, photo, ruby, #, alaska, smoke, #, ...</td>\n","      <td>[got, sent, photo, rubi, #, alaska, smoke, #, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                     porter_stemmer\n","0   1  ...  [deed, reason, #, earthquak, may, allah, forgi...\n","1   4  ...    [forest, fire, near, la, rong, sask, ., canada]\n","2   5  ...  [resid, ask, 'shelter, place, ', notifi, offic...\n","3   6  ...  [13,000, peopl, receiv, #, wildfir, evacu, ord...\n","4   7  ...  [got, sent, photo, rubi, #, alaska, smoke, #, ...\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"AWCTp3wXWmUF"},"source":["from nltk.stem import SnowballStemmer\n","\n","def snowball_stemmer(text):\n","    \"\"\"\n","        Stem words in list of tokenized words with SnowballStemmer\n","    \"\"\"\n","    stemmer = nltk.SnowballStemmer(\"english\")\n","    stems = [stemmer.stem(i) for i in text]\n","    return stems"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"OwBe_w10Wnwy","executionInfo":{"status":"ok","timestamp":1625914388582,"user_tz":-270,"elapsed":851,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"c98b429a-c371-4ad9-f018-ea3f1bdb2d17"},"source":["%time \n","\n","train_df['snowball_stemmer'] = train_df['stopwords_removed'].apply(lambda x: snowball_stemmer(x))\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n","Wall time: 6.44 µs\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","      <th>stopwords_removed</th>\n","      <th>porter_stemmer</th>\n","      <th>snowball_stemmer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","      <td>[our, deeds, are, the, reason, of, this, #, ea...</td>\n","      <td>[deeds, reason, #, earthquake, may, allah, for...</td>\n","      <td>[deed, reason, #, earthquak, may, allah, forgi...</td>\n","      <td>[deed, reason, #, earthquak, may, allah, forgi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, rong, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, rong, sask, ., canada]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","      <td>[all, residents, asked, to, 'shelter, in, plac...</td>\n","      <td>[residents, asked, 'shelter, place, ', notifie...</td>\n","      <td>[resid, ask, 'shelter, place, ', notifi, offic...</td>\n","      <td>[resid, ask, shelter, place, ', notifi, offic,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","      <td>[13,000, peopl, receiv, #, wildfir, evacu, ord...</td>\n","      <td>[13,000, peopl, receiv, #, wildfir, evacu, ord...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, #, ...</td>\n","      <td>[got, sent, photo, ruby, #, alaska, smoke, #, ...</td>\n","      <td>[got, sent, photo, rubi, #, alaska, smoke, #, ...</td>\n","      <td>[got, sent, photo, rubi, #, alaska, smoke, #, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                   snowball_stemmer\n","0   1  ...  [deed, reason, #, earthquak, may, allah, forgi...\n","1   4  ...    [forest, fire, near, la, rong, sask, ., canada]\n","2   5  ...  [resid, ask, shelter, place, ', notifi, offic,...\n","3   6  ...  [13,000, peopl, receiv, #, wildfir, evacu, ord...\n","4   7  ...  [got, sent, photo, rubi, #, alaska, smoke, #, ...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"0Pb1GQcKWpZI"},"source":["from nltk.stem import LancasterStemmer\n","\n","def lancaster_stemmer(text):\n","    \"\"\"\n","        Stem words in list of tokenized words with LancasterStemmer\n","    \"\"\"\n","    stemmer = nltk.LancasterStemmer()\n","    stems = [stemmer.stem(i) for i in text]\n","    return stems\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":557},"id":"LWvxuOExWq4-","executionInfo":{"status":"ok","timestamp":1625914394443,"user_tz":-270,"elapsed":2535,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"aca9dfcb-3d5a-44be-b443-f4dd60c83c0f"},"source":["%time \n","\n","train_df['lancaster_stemmer'] = train_df['stopwords_removed'].apply(lambda x: lancaster_stemmer(x))\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n","Wall time: 21.2 µs\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","      <th>stopwords_removed</th>\n","      <th>porter_stemmer</th>\n","      <th>snowball_stemmer</th>\n","      <th>lancaster_stemmer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","      <td>[our, deeds, are, the, reason, of, this, #, ea...</td>\n","      <td>[deeds, reason, #, earthquake, may, allah, for...</td>\n","      <td>[deed, reason, #, earthquak, may, allah, forgi...</td>\n","      <td>[deed, reason, #, earthquak, may, allah, forgi...</td>\n","      <td>[dee, reason, #, earthquak, may, allah, forg, us]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, ronge, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, rong, sask, ., canada]</td>\n","      <td>[forest, fire, near, la, rong, sask, ., canada]</td>\n","      <td>[forest, fir, near, la, rong, sask, ., canad]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","      <td>[all, residents, asked, to, 'shelter, in, plac...</td>\n","      <td>[residents, asked, 'shelter, place, ', notifie...</td>\n","      <td>[resid, ask, 'shelter, place, ', notifi, offic...</td>\n","      <td>[resid, ask, shelter, place, ', notifi, offic,...</td>\n","      <td>[resid, ask, 'shelter, plac, ', not, off, ., e...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n","      <td>[13,000, peopl, receiv, #, wildfir, evacu, ord...</td>\n","      <td>[13,000, peopl, receiv, #, wildfir, evacu, ord...</td>\n","      <td>[13,000, peopl, receiv, #, wildfir, evacu, ord...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, #, ...</td>\n","      <td>[got, sent, photo, ruby, #, alaska, smoke, #, ...</td>\n","      <td>[got, sent, photo, rubi, #, alaska, smoke, #, ...</td>\n","      <td>[got, sent, photo, rubi, #, alaska, smoke, #, ...</td>\n","      <td>[got, sent, photo, ruby, #, alask, smok, #, wi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                  lancaster_stemmer\n","0   1  ...  [dee, reason, #, earthquak, may, allah, forg, us]\n","1   4  ...      [forest, fir, near, la, rong, sask, ., canad]\n","2   5  ...  [resid, ask, 'shelter, plac, ', not, off, ., e...\n","3   6  ...  [13,000, peopl, receiv, #, wildfir, evacu, ord...\n","4   7  ...  [got, sent, photo, ruby, #, alask, smok, #, wi...\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"y9CfCSQ_WtgW"},"source":["from nltk.corpus import wordnet\n","from nltk.corpus import brown\n","\n","wordnet_map = {\"N\":wordnet.NOUN, \n","               \"V\":wordnet.VERB, \n","               \"J\":wordnet.ADJ, \n","               \"R\":wordnet.ADV\n","              }\n","    \n","train_sents = brown.tagged_sents(categories='news')\n","t0 = nltk.DefaultTagger('NN')\n","t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n","t2 = nltk.BigramTagger(train_sents, backoff=t1)\n","\n","def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n","    \"\"\"\n","        Create pos_tag with wordnet format\n","    \"\"\"\n","    pos_tagged_text = t2.tag(text)\n","    \n","    # map the pos tagging output with wordnet output \n","    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n","    return pos_tagged_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T5OYMvkXWuxZ"},"source":["pos_tag_wordnet(train_df['stopwords_removed'][2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XhlGGnvdWwdg"},"source":["%time \n","\n","train_df['combined_postag_wnet'] = train_df['stopwords_removed'].apply(lambda x: pos_tag_wordnet(x))\n","\n","train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2socXqepXJTf"},"source":["%time \n","\n","train_df['combined_postag_wnet'] = train_df['stopwords_removed'].apply(lambda x: pos_tag_wordnet(x))\n","\n","train_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lm1MfEBGXLDV"},"source":["from nltk.stem import WordNetLemmatizer\n","\n","def lemmatize_word(text):\n","    \"\"\"\n","        Lemmatize the tokenized words\n","    \"\"\"\n","\n","    lemmatizer = WordNetLemmatizer()\n","    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n","    return lemma\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlv4f1IjXMnl"},"source":["\n","\n","%time \n","\n","# Test without POS Tagging\n","lemmatizer = WordNetLemmatizer()\n","\n","train_df['lemmatize_word_wo_pos'] = train_df['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","train_df['lemmatize_word_wo_pos'] = train_df['lemmatize_word_wo_pos'].apply(lambda x: [word for word in x if word not in stop])\n","train_df.head()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TB_qy8K6XOoU"},"source":["\n","\n","print(train_df[\"combined_postag_wnet\"][8])\n","print(train_df[\"lemmatize_word_wo_pos\"][8])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ympSpG8rXQMP"},"source":["%time \n","\n","# Test with POS Tagging\n","lemmatizer = WordNetLemmatizer()\n","\n","train_df['lemmatize_word_w_pos'] = train_df['combined_postag_wnet'].apply(lambda x: lemmatize_word(x))\n","train_df['lemmatize_word_w_pos'] = train_df['lemmatize_word_w_pos'].apply(lambda x: [word for word in x if word not in stop]) # double check to remove stop words\n","train_df['lemmatize_text'] = [' '.join(map(str, l)) for l in train_df['lemmatize_word_w_pos']] # join back to text\n","\n","train_df.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"piVd47zCXSTl"},"source":["print(train_df[\"text\"][8])\n","print(train_df[\"combined_postag_wnet\"][8])\n","print(train_df[\"lemmatize_word_wo_pos\"][8])\n","print(train_df[\"lemmatize_word_w_pos\"][8])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oo30UIskXT_K"},"source":["display(train_df[\"text\"][0], train_df[\"lemmatize_text\"][0])\n","display(train_df[\"text\"][5], train_df[\"lemmatize_text\"][5])\n","display(train_df[\"text\"][10], train_df[\"lemmatize_text\"][10])\n","display(train_df[\"text\"][15], train_df[\"lemmatize_text\"][15])\n","display(train_df[\"text\"][20], train_df[\"lemmatize_text\"][20])\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uERhhko-eCQR"},"source":["# Word embedings "]},{"cell_type":"code","metadata":{"id":"mcaGtktveEhg"},"source":["%time \n","\n","import gensim\n","print(\"gensim version:\", gensim.__version__)\n","\n","word2vec_path = \"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\"\n","\n","# we only load 200k most common words from Google News corpus \n","word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=200000) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJoL8c9VeG3q"},"source":["\n","\n","print(word2vec_model.similarity('cat', 'kitten'))\n","print(word2vec_model.similarity('cat', 'cats'))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0r2JdtAgeIvZ"},"source":["def get_average_vec(tokens_list, vector, generate_missing=False, k=300):\n","    \"\"\"\n","        Calculate average embedding value of sentence from each word vector\n","    \"\"\"\n","    \n","    if len(tokens_list)<1:\n","        return np.zeros(k)\n","    \n","    if generate_missing:\n","        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n","    else:\n","        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n","    \n","    length = len(vectorized)\n","    summed = np.sum(vectorized, axis=0)\n","    averaged = np.divide(summed, length)\n","    return averaged\n","\n","def get_embeddings(vectors, text, generate_missing=False, k=300):\n","    \"\"\"\n","        create the sentence embedding\n","    \"\"\"\n","    embeddings = text.apply(lambda x: get_average_vec(x, vectors, generate_missing=generate_missing, k=k))\n","    return list(embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1Z6230ReKC4"},"source":["%time \n","\n","embeddings_word2vec = get_embeddings(word2vec_model, train_df[\"lemmatize_text\"], k=300)\n","\n","print(\"Embedding matrix size\", len(embeddings_word2vec), len(embeddings_word2vec[0]))\n","print(\"The sentence: \\\"%s\\\" got embedding values: \" % train_df[\"lemmatize_text\"][0])\n","print(embeddings_word2vec[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFp0GHQHeL1n"},"source":["del embeddings_word2vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hz7MDRXAeNbg"},"source":["%time \n","\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","\n","glove_input_file = \"../input/glove6b/glove.6B.300d.txt\"\n","word2vec_output_file = \"glove.6B.100d.txt.word2vec\"\n","glove2word2vec(glove_input_file, word2vec_output_file)\n","\n","# we only load 200k most common words from Google New corpus \n","glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False, limit=200000) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ubp089SAePCY"},"source":["print(glove_model.similarity('cat', 'kitten'))\n","print(glove_model.similarity('cat', 'cats'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IN1u3vUKeQJZ"},"source":["%time \n","\n","embeddings_glove = get_embeddings(glove_model, train_df[\"lemmatize_text\"], k=300)\n","\n","print(\"Embedding matrix size\", len(embeddings_glove), len(embeddings_glove[0]))\n","print(\"The sentence: \\\"%s\\\" got embedding values: \" % train_df[\"lemmatize_text\"][0])\n","print(embeddings_glove[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zaNQ5jN6eRv_"},"source":["del embeddings_glove\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfZ61WdtLZbO"},"source":["# NEW WAY"]},{"cell_type":"code","metadata":{"id":"Z0zo4vZ4Lb4j","executionInfo":{"status":"ok","timestamp":1625997100718,"user_tz":-270,"elapsed":346,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["import pandas as pd\n","from keras.preprocessing import sequence\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import train_test_split\n","\n","class Preprocessing:\n","\t\n","\tdef __init__(self,data_path):\n","\t\tself.data = data_path\n","\t\tself.max_len = 10\n","\t\tself.max_words = 100\n","\t\tself.test_size = 0.5\n","\t\t\n","\tdef load_data(self):\n","\t\tdf = pd.read_csv(self.data)\n","\t\tdf.drop(['id','keyword','location'], axis=1, inplace=True)\n","\t\t\n","\t\tX = df['text'].values\n","\t\tY = df['target'].values\n","\t\t\n","\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n","\t\t\n","\tdef prepare_tokens(self):\n","\t\tself.tokens = Tokenizer(num_words=self.max_words)\n","\t\tself.tokens.fit_on_texts(self.x_train)\n","\n","\tdef sequence_to_token(self, x):\n","\t\tsequences = self.tokens.texts_to_sequences(x)\n","\t\treturn sequence.pad_sequences(sequences, maxlen=self.max_len)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"1uStwICRCdHd","executionInfo":{"status":"ok","timestamp":1625998425634,"user_tz":-270,"elapsed":293,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["preprocessing = Preprocessing('./data/train.csv')\n","preprocessing.load_data()\n","preprocessing.prepare_tokens()\n","\n","raw_x_train = preprocessing.x_train\n","raw_x_test = preprocessing.x_test\n","\n","y_train = preprocessing.y_train\n","y_test = preprocessing.y_test\n","\n","x_train = preprocessing.sequence_to_token(raw_x_train)\n","x_test = preprocessing.sequence_to_token(raw_x_test)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"ElF81zZgLeEh","executionInfo":{"status":"ok","timestamp":1625998662286,"user_tz":-270,"elapsed":299,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class TweetClassifier(nn.ModuleList):\n","\n","\tdef __init__(self, batch_size=64, hidden_dim=10, lstm_layers=3, max_words= 50):\n","\t\tsuper(TweetClassifier, self).__init__()\n","\t\t\n","\t\t# Hyperparameters\n","\t\tself.batch_size =batch_size\n","\t\tself.hidden_dim = hidden_dim\n","\t\tself.LSTM_layers = lstm_layers\n","\t\tself.input_size = max_words\n","\t\t\n","\t\tself.dropout = nn.Dropout(0.5)\n","\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n","\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n","\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim*2)\n","\t\tself.fc2 = nn.Linear(self.hidden_dim*2, 1)\n","\t\t\n","\tdef forward(self, x):\n","\t\t\n","\t\t# Hidden and cell state definion\n","\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n","\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n","\t\t\n","\t\t# Initialization fo hidden and cell states\n","\t\ttorch.nn.init.xavier_normal_(h)\n","\t\ttorch.nn.init.xavier_normal_(c)\n","\n","\t\t# Each sequence \"x\" is passed through an embedding layer\n","\t\tout = self.embedding(x)\n","\t\t# Feed LSTMs\n","\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n","\t\tout = self.dropout(out)\n","\t\t# The last hidden state is taken\n","\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n","\t\tout = self.dropout(out)\n","\t\tout = torch.sigmoid(self.fc2(out))\n","\n","\t\treturn out"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfFRNiXHLpLd","executionInfo":{"status":"ok","timestamp":1625998446650,"user_tz":-270,"elapsed":304,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class DatasetMaper(Dataset):\n","\t'''\n","\tHandles batches of dataset\n","\t'''\n","  \n","\tdef __init__(self, x, y):\n","\t\tself.x = x\n","\t\tself.y = y\n","\t\t\n","\tdef __len__(self):\n","\t\treturn len(self.x)\n","\t\t\n","\tdef __getitem__(self, idx):\n","\t\treturn self.x[idx], self.y[idx]\n","\n","training_set = DatasetMaper(x_train, y_train)\n","test_set = DatasetMaper(x_test, y_test)\n","\t\t\n","loader_training = DataLoader(training_set, batch_size=64)\n","loader_test = DataLoader(test_set)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"xgF_Jv9YD_eH","executionInfo":{"status":"ok","timestamp":1625998667351,"user_tz":-270,"elapsed":353,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["model = TweetClassifier()"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdewDRs5Lq6s","colab":{"base_uri":"https://localhost:8080/","height":959},"executionInfo":{"status":"error","timestamp":1625998846824,"user_tz":-270,"elapsed":305,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"6787e63b-36a4-4ac7-fac9-3146c3326b0d"},"source":["# Defines a RMSprop optimizer to update the parameters\n","optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n","\n","for epoch in range(5):\n","\n","  predictions = []\n","\n","  # model in training mode\n","  model.train()\n","\n","  for x_batch, y_batch in loader_training:\n","\n","    x = x_batch.type(torch.LongTensor)\n","    y = y_batch.type(torch.FloatTensor)\n","\n","    # Feed the model and get output \"y_pred\"\n","    y_pred = model(x)\n","\n","    # Calculate loss\n","    loss = F.binary_cross_entropy(y_pred, y)\n","\n","    # The gradientes are calculated\n","    # i.e. derivates are calculated\n","    loss.backward()\n","    \n","    # Each parameter is updated\n","    # with torch.no_grad():\n","    #     a -= lr * a.grad\n","    #     b -= lr * b.grad\n","    optimizer.step()\n","    \n","    # Take the gradients to zero!\n","    # a.grad.zero_()\n","    # b.grad.zero_()\n","    optimizer.zero_grad()"],"execution_count":32,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-c82689292cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Feed the model and get output \"y_pred\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-9cdd7d367739>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m# Each sequence \"x\" is passed through an embedding layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# Feed LSTMs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}]},{"cell_type":"code","metadata":{"id":"Tm2EXHYDLynu","executionInfo":{"status":"ok","timestamp":1625998835880,"user_tz":-270,"elapsed":322,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["def evaluation():\n","\n","\t\tpredictions = []\n","    \n","    \t\t# The model is turned in evaluation mode\n","\t\tmodel.eval()\n","    \n","        \t# Skipping gradients update\n","\t\twith torch.no_grad():\n","      \n","            \t\t# Iterate over the DataLoader object\n","\t\t\tfor x_batch, y_batch in loader_test:\n","        \n","\t\t\t\tx = x_batch.type(torch.LongTensor)\n","\t\t\t\ty = y_batch.type(torch.FloatTensor)\n","\t\t\t\t\n","                \t\t# Feed the model\n","\t\t\t\ty_pred = model(x)\n","        \n","                \t\t# Save prediction\n","\t\t\t\tpredictions += list(y_pred.detach().numpy())\n","\t\t\t\t\n","\t\treturn predictions"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDgp40v6L174"},"source":["def calculate_accuray(grand_truth, predictions):\n","  \n","\t\ttrue_positives = 0\n","\t\ttrue_negatives = 0\n","\t\t\n","\t\tfor true, pred in zip(grand_truth, predictions):\n","\t\t\tif (pred > 0.5) and (true == 1):\n","\t\t\t\ttrue_positives += 1\n","\t\t\telif (pred < 0.5) and (true == 0):\n","\t\t\t\ttrue_negatives += 1\n","\t\t\telse:\n","\t\t\t\tpass\n","\t\t\t\t\n","\t\treturn (true_positives+true_negatives) / len(grand_truth)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lm8JXqoCOt9x"},"source":["# BERT Usage "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmZTVSYTOxGa","executionInfo":{"status":"ok","timestamp":1625998981960,"user_tz":-270,"elapsed":8510,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"e1abd349-9223-48a7-faca-fa33cdba9b51"},"source":["!pip install transformers\n","!pip install -U torchtext\n","# Libraries\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import torch\n","\n","# Preliminaries\n","\n","from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n","\n","# Models\n","\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# Training\n","\n","import torch.optim as optim\n","\n","# Evaluation\n","\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import seaborn as sns"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 7.3MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 37.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 47.1MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n","Requirement already up-to-date: torchtext in /usr/local/lib/python3.7/dist-packages (0.10.0)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n","Requirement already satisfied, skipping upgrade: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.9.0+cu102)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.5.30)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YaBZ3n6ySRhq","executionInfo":{"status":"ok","timestamp":1625998984915,"user_tz":-270,"elapsed":337,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["source_folder = './data'\n","destination_folder = './log'"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUuexotZS-rZ","executionInfo":{"status":"ok","timestamp":1625998998988,"user_tz":-270,"elapsed":395,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"5ad44111-e8df-4c2a-ed9c-717276d0456a"},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","print(device)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"AQ8vfoPLUifd","executionInfo":{"status":"ok","timestamp":1625998999506,"user_tz":-270,"elapsed":25,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"62d13aad-32d0-400e-da76-45b829163242"},"source":["train_df.head()"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ...                                               text target\n","0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n","1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n","2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n","3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n","4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"Ch88PUFdPCCm","colab":{"base_uri":"https://localhost:8080/","height":170,"referenced_widgets":["8a1638675f23465cba1f8aec2c08f263","0bc59874be2c4dd98ebe54fb51897b58","40e61ff8a63f44f5aaf590fbc06c775f","2e48ec49a30441cbad04693b7284bb81","73cfc9a5511a41eabecfd0c0c858db62","7a64509fcaab49fcaf2dc4748afba5a1","dcfadb8dd48d40fbb75cd0b24454a54e","16d4ff792cd847eb83aa79087247e4d9","66db64fdeb104aae975d05e332809a83","439a13186bb3484a86697cc9c0ca3acf","de268f38d646408286d3d212b07fcff5","18d39178a55945868f3be088ca996e9e","7722741f4df449c6863099a1ceff6048","b00882c2be314f90a5573038ba611d58","5df7470d21f742be9bd94442196a510c","c09c95425c79413a959b48062d12e90b","8fb869b3f3f54e8d951742cf0d8dc4c5","d09a56b774644c21aa069ee218ce71d3","97f034f95b2b450b9278d4716084dd62","e16f4778ceda4faca18ce170685cd37e","c5b6e4dccb1c4ac0aaace30a13830cd1","9590236ea9c64b6ba80abc61deade33e","1a9147fca45b4bc9afbcf88d5e574b87","6a987fee8ebb403ab9324e9b0ae3585b"]},"executionInfo":{"status":"ok","timestamp":1625999011246,"user_tz":-270,"elapsed":11758,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"6184fa1c-a906-4e01-b723-8f3bff69625b"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Model parameter\n","MAX_SEQ_LEN = 128\n","PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n","UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n","\n","# Fields\n","\n","label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n","text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n","                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n","fields = [('id', label_field), ('keyword', text_field), ('location', text_field) ,('text', text_field), ('target', label_field)]\n","\n","# TabularDataset\n","\n","train, test = TabularDataset.splits(path=source_folder, train='train.csv',test='test.csv',\n","                                    format='CSV', fields=fields, skip_header=True)\n","\n","# Iterators\n","\n","train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),\n","                            device=device, train=True, sort=True, sort_within_batch=True)\n","\n","test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)"],"execution_count":38,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a1638675f23465cba1f8aec2c08f263","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66db64fdeb104aae975d05e332809a83","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fb869b3f3f54e8d951742cf0d8dc4c5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GANJyL3nT3hT","executionInfo":{"status":"ok","timestamp":1625999011591,"user_tz":-270,"elapsed":17,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"d841bca7-4eee-4494-eb3b-2bd3f6098e72"},"source":["train_iter"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torchtext.legacy.data.iterator.BucketIterator at 0x7fa375e70a90>"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"fQjkLuswPFpA","executionInfo":{"status":"ok","timestamp":1625999011594,"user_tz":-270,"elapsed":15,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["class BERT(nn.Module):\n","\n","    def __init__(self):\n","        super(BERT, self).__init__()\n","\n","        options_name = \"bert-base-uncased\"\n","        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n","\n","    def forward(self, text, label):\n","        loss, text_fea = self.encoder(text, labels=label)[:2]\n","\n","        return loss, text_fea"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"466tT-VTPKFL","executionInfo":{"status":"ok","timestamp":1625999011596,"user_tz":-270,"elapsed":14,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}}},"source":["# Save and Load Functions\n","\n","def save_checkpoint(save_path, model, valid_loss):\n","\n","    if save_path == None:\n","        return\n","    \n","    state_dict = {'model_state_dict': model.state_dict(),\n","                  'valid_loss': valid_loss}\n","    \n","    torch.save(state_dict, save_path)\n","    print(f'Model saved to ==> {save_path}')\n","\n","def load_checkpoint(load_path, model):\n","    \n","    if load_path==None:\n","        return\n","    \n","    state_dict = torch.load(load_path, map_location=device)\n","    print(f'Model loaded from <== {load_path}')\n","    \n","    model.load_state_dict(state_dict['model_state_dict'])\n","    return state_dict['valid_loss']\n","\n","\n","def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n","\n","    if save_path == None:\n","        return\n","    \n","    state_dict = {'train_loss_list': train_loss_list,\n","                  'valid_loss_list': valid_loss_list,\n","                  'global_steps_list': global_steps_list}\n","    \n","    torch.save(state_dict, save_path)\n","    print(f'Model saved to ==> {save_path}')\n","\n","\n","def load_metrics(load_path):\n","\n","    if load_path==None:\n","        return\n","    \n","    state_dict = torch.load(load_path, map_location=device)\n","    print(f'Model loaded from <== {load_path}')\n","    \n","    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":902},"id":"lQhQ9sh2PPFX","executionInfo":{"status":"error","timestamp":1626007653012,"user_tz":-270,"elapsed":2969222,"user":{"displayName":"Amin DM","photoUrl":"","userId":"12964775003138613495"}},"outputId":"06064284-bfd5-4ed7-8dac-1dea03927909"},"source":["# Training Function\n","\n","def train(model,\n","          optimizer,\n","          criterion = nn.BCELoss(),\n","          train_loader = train_iter,\n","          valid_loader = train_iter,\n","          num_epochs = 50,\n","          eval_every = len(train_iter) // 2,\n","          file_path = destination_folder,\n","          best_valid_loss = float(\"Inf\")):\n","    \n","    # initialize running values\n","    running_loss = 0.0\n","    valid_running_loss = 0.0\n","    global_step = 0\n","    train_loss_list = []\n","    valid_loss_list = []\n","    global_steps_list = []\n","\n","    # training loop\n","    model.train()\n","    for epoch in range(num_epochs):\n","        for (id, keyword, location, text, labels), _ in train_loader:\n","            labels = labels.type(torch.LongTensor)           \n","            labels = labels.to(device)\n","            text = text.type(torch.LongTensor)  \n","            text = text.to(device)\n","            output = model(text, labels)\n","            loss, _ = output\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # update running values\n","            running_loss += loss.item()\n","            global_step += 1\n","\n","            # evaluation step\n","            if global_step % eval_every == 0:\n","                model.eval()\n","                with torch.no_grad():                    \n","\n","                    # validation loop\n","                    for (id, keyword, location, text, labels), _ in valid_loader:\n","                        labels = labels.type(torch.LongTensor)           \n","                        labels = labels.to(device)\n","                        text = text.type(torch.LongTensor)  \n","                        text = text.to(device)\n","                        output = model(text, labels)\n","                        loss, _ = output\n","                        \n","                        valid_running_loss += loss.item()\n","\n","                # evaluation\n","                average_train_loss = running_loss / eval_every\n","                average_valid_loss = valid_running_loss / len(valid_loader)\n","                train_loss_list.append(average_train_loss)\n","                valid_loss_list.append(average_valid_loss)\n","                global_steps_list.append(global_step)\n","\n","                # resetting running values\n","                running_loss = 0.0                \n","                valid_running_loss = 0.0\n","                model.train()\n","\n","                # print progress\n","                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n","                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n","                              average_train_loss, average_valid_loss))\n","                \n","                # checkpoint\n","                if best_valid_loss > average_valid_loss:\n","                    best_valid_loss = average_valid_loss\n","                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n","                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n","    \n","    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n","    print('Finished Training!')\n","\n","model = BERT().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","train(model=model, optimizer=optimizer)"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch [1/50], Step [238/23800], Train Loss: 0.6201, Valid Loss: 0.6340\n","Model saved to ==> ./log/model.pt\n","Model saved to ==> ./log/metrics.pt\n","Epoch [2/50], Step [476/23800], Train Loss: 0.6438, Valid Loss: 0.7183\n","Epoch [3/50], Step [714/23800], Train Loss: 0.6461, Valid Loss: 0.7107\n","Epoch [4/50], Step [952/23800], Train Loss: 0.6459, Valid Loss: 0.7302\n","Epoch [5/50], Step [1190/23800], Train Loss: 0.6529, Valid Loss: 0.7407\n","Epoch [6/50], Step [1428/23800], Train Loss: 0.6484, Valid Loss: 0.7243\n","Epoch [7/50], Step [1666/23800], Train Loss: 0.6506, Valid Loss: 0.7196\n","Epoch [8/50], Step [1904/23800], Train Loss: 0.6549, Valid Loss: 0.7180\n","Epoch [9/50], Step [2142/23800], Train Loss: 0.6546, Valid Loss: 0.7172\n","Epoch [10/50], Step [2380/23800], Train Loss: 0.6504, Valid Loss: 0.7127\n","Epoch [11/50], Step [2618/23800], Train Loss: 0.6563, Valid Loss: 0.7098\n","Epoch [12/50], Step [2856/23800], Train Loss: 0.6480, Valid Loss: 0.7106\n","Epoch [13/50], Step [3094/23800], Train Loss: 0.6512, Valid Loss: 0.7101\n","Epoch [14/50], Step [3332/23800], Train Loss: 0.6517, Valid Loss: 0.7127\n","Epoch [15/50], Step [3570/23800], Train Loss: 0.6508, Valid Loss: 0.7114\n","Epoch [16/50], Step [3808/23800], Train Loss: 0.6515, Valid Loss: 0.7094\n","Epoch [17/50], Step [4046/23800], Train Loss: 0.6503, Valid Loss: 0.7116\n","Epoch [18/50], Step [4284/23800], Train Loss: 0.6494, Valid Loss: 0.7150\n","Epoch [19/50], Step [4522/23800], Train Loss: 0.6500, Valid Loss: 0.7120\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-7618a49ad603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-44-7618a49ad603>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_loader, valid_loader, num_epochs, eval_every, file_path, best_valid_loss)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"g6tlXUXrPSP1"},"source":["train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n","plt.plot(global_steps_list, train_loss_list, label='Train')\n","plt.plot(global_steps_list, valid_loss_list, label='Valid')\n","plt.xlabel('Global Steps')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXjyFGa7PUrO"},"source":["# Evaluation Function\n","\n","def evaluate(model, test_loader):\n","    y_pred = []\n","    y_true = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for (labels, title, text, titletext), _ in test_loader:\n","\n","                labels = labels.type(torch.LongTensor)           \n","                labels = labels.to(device)\n","                titletext = titletext.type(torch.LongTensor)  \n","                titletext = titletext.to(device)\n","                output = model(titletext, labels)\n","\n","                _, output = output\n","                y_pred.extend(torch.argmax(output, 1).tolist())\n","                y_true.extend(labels.tolist())\n","    \n","    print('Classification Report:')\n","    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n","    \n","    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n","    ax= plt.subplot()\n","    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n","\n","    ax.set_title('Confusion Matrix')\n","\n","    ax.set_xlabel('Predicted Labels')\n","    ax.set_ylabel('True Labels')\n","\n","    ax.xaxis.set_ticklabels(['FAKE', 'REAL'])\n","    ax.yaxis.set_ticklabels(['FAKE', 'REAL'])\n","    \n","best_model = BERT().to(device)\n","\n","load_checkpoint(destination_folder + '/model.pt', best_model)\n","\n","evaluate(best_model, test_iter)"],"execution_count":null,"outputs":[]}]}